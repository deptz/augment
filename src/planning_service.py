"""
Planning Service for Epic Development Planning
"""
import logging
import re
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta

from .planning_models import (
    EpicPlan, StoryPlan, TaskPlan, CycleTimeEstimate, AcceptanceCriteria,
    TestCase, PlanningResult, OperationMode, PlanningContext, GapAnalysis,
    TaskScope, TaskTeam
)
from .epic_analysis_engine import EpicAnalysisEngine
from .jira_client import JiraClient
from .llm_client import LLMClient
from .bulk_ticket_creator import BulkTicketCreator
from .confluence_client import ConfluenceClient
from .planning_prompt_engine import PlanningPromptEngine, DocumentType
from .enhanced_test_generator import EnhancedTestGenerator, TestCoverageLevel
from .team_based_task_generator import TeamBasedTaskGenerator
from .prompts import Prompts

logger = logging.getLogger(__name__)


class PlanningService:
    """Core service for planning operations and ticket creation"""
    
    def __init__(
        self, 
        jira_client: JiraClient, 
        confluence_client: ConfluenceClient,
        llm_client: LLMClient
    ):
        self.jira_client = jira_client
        self.confluence_client = confluence_client
        self.llm_client = llm_client
        self.analysis_engine = EpicAnalysisEngine(jira_client, confluence_client)
        self.bulk_creator = BulkTicketCreator(jira_client, confluence_client)
        self.prompt_engine = PlanningPromptEngine()
        # Enhanced test generator with full context access
        self.test_generator = EnhancedTestGenerator(
            llm_client, 
            self.prompt_engine, 
            jira_client=jira_client, 
            confluence_client=confluence_client
        )
        self.team_task_generator = TeamBasedTaskGenerator(llm_client, self.prompt_engine)
        
        # Task relationship tracking
        self._task_summary_to_plan = {}
        self._task_summary_to_key = {}
    
    def plan_epic_complete(self, context: PlanningContext) -> PlanningResult:
        """
        Complete planning for an epic - generate all missing stories and tasks
        
        Args:
            context: Planning context with epic key and options
            
        Returns:
            PlanningResult with generated plan and execution details
        """
        start_time = time.time()
        logger.info(f"Starting complete epic planning for {context.epic_key}")
        
        try:
            # Step 1: Analyze current epic structure
            gap_analysis = self.analysis_engine.analyze_epic_structure(context.epic_key)
            context.gap_analysis = gap_analysis
            
            # Step 2: Generate complete epic plan
            epic_plan = self._generate_epic_plan(context)
            context.epic_plan = epic_plan
            
            # Step 3: Create tickets if not dry run
            created_tickets = {}
            if not context.dry_run:
                created_tickets = self._create_epic_tickets(epic_plan)
            
            # Step 4: Generate summary statistics
            summary_stats = epic_plan.get_summary_stats() if epic_plan else {}
            
            execution_time = time.time() - start_time
            
            result = PlanningResult(
                epic_key=context.epic_key,
                mode=context.mode,
                success=True,
                created_tickets=created_tickets,
                gap_analysis=gap_analysis,
                epic_plan=epic_plan,
                summary_stats=summary_stats,
                execution_time_seconds=execution_time
            )
            
            logger.info(f"Epic planning completed for {context.epic_key} in {execution_time:.2f}s")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Error in epic planning for {context.epic_key}: {str(e)}")
            
            return PlanningResult(
                epic_key=context.epic_key,
                mode=context.mode,
                success=False,
                errors=[str(e)],
                execution_time_seconds=execution_time
            )
    
    def generate_stories_for_epic(self, context: PlanningContext) -> PlanningResult:
        """
        Generate only stories for an epic (without tasks)
        
        Args:
            context: Planning context
            
        Returns:
            PlanningResult with generated stories
        """
        start_time = time.time()
        logger.info(f"Generating stories for epic {context.epic_key}")
        
        try:
            # Get epic information
            epic_issue = self.jira_client.jira.issue(context.epic_key)
            epic_description_field = epic_issue.fields.description
            
            # Extract epic description text
            if isinstance(epic_description_field, dict):
                epic_description = self._extract_text_from_adf(epic_description_field)
            else:
                epic_description = epic_description_field or ''
            
            # Get PRD/RFC content
            prd_content = self._get_prd_content(epic_issue)
            rfc_content = self._get_rfc_content(epic_issue)
            
            # Analyze gaps
            gap_analysis = self.analysis_engine.analyze_epic_structure(context.epic_key)
            
            # Generate stories only
            stories = self._generate_stories_for_epic_plan(context.epic_key, epic_description, gap_analysis, prd_content, rfc_content)
            
            epic_plan = EpicPlan(
                epic_key=context.epic_key,
                epic_title=f"Epic {context.epic_key}",
                stories=stories
            )
            
            # Create story tickets if not dry run
            created_tickets = {}
            if not context.dry_run:
                created_tickets = self._create_story_tickets(stories)
            
            execution_time = time.time() - start_time
            
            return PlanningResult(
                epic_key=context.epic_key,
                mode=context.mode,
                success=True,
                created_tickets=created_tickets,
                gap_analysis=gap_analysis,
                epic_plan=epic_plan,
                execution_time_seconds=execution_time
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Error generating stories for {context.epic_key}: {str(e)}")
            
            return PlanningResult(
                epic_key=context.epic_key,
                mode=context.mode,
                success=False,
                errors=[str(e)],
                execution_time_seconds=execution_time
            )
    
    def generate_tasks_for_stories(self, story_keys: List[str], context: PlanningContext, custom_llm_client: Optional['LLMClient'] = None) -> PlanningResult:
        """
        Generate tasks for specific stories
        
        Args:
            story_keys: List of story keys to generate tasks for
            context: Planning context
            custom_llm_client: Optional custom LLM client to use for generation
            
        Returns:
            PlanningResult with generated tasks
        """
        start_time = time.time()
        logger.info(f"Generating tasks for {len(story_keys)} stories")
        
        try:
            all_tasks = []
            created_tickets = {"tasks": []}
            
            for story_key in story_keys:
                # Generate tasks for this story
                tasks = self._generate_tasks_for_story(story_key, context, custom_llm_client)
                all_tasks.extend(tasks)
                
                # Create task tickets if not dry run
                if not context.dry_run:
                    logger.info(f"Dry run is FALSE - will create {len(tasks)} JIRA tickets for story {story_key}")
                    task_tickets = self._create_task_tickets(tasks)
                    created_tickets["tasks"].extend(task_tickets)
                    logger.info(f"Created {len(task_tickets)} task tickets for story {story_key}: {task_tickets}")
                else:
                    logger.info(f"Dry run is TRUE - not creating actual JIRA tickets for story {story_key}")
                    # In dry run mode, include task summaries for preview
                    task_summaries = [f"[DRY_RUN] {task.summary}" for task in tasks]
                    created_tickets["tasks"].extend(task_summaries)
                    logger.debug(f"Dry run task summaries: {task_summaries}")
            
            # Create epic plan with stories containing the generated tasks
            stories_with_tasks = []
            tasks_by_story = {}
            
            # Group tasks by story
            for task in all_tasks:
                story_key = task.story_key or "unknown"
                if story_key not in tasks_by_story:
                    tasks_by_story[story_key] = []
                tasks_by_story[story_key].append(task)
            
            # Create StoryPlan objects with tasks
            for story_key, tasks in tasks_by_story.items():
                story_plan = StoryPlan(
                    summary=f"Story {story_key}",
                    description=f"Story for {story_key}",
                    acceptance_criteria=[],
                    tasks=tasks,
                    epic_key=context.epic_key
                )
                stories_with_tasks.append(story_plan)
            
            epic_plan = EpicPlan(
                epic_key=context.epic_key,
                epic_title=f"Epic {context.epic_key}",
                stories=stories_with_tasks  # Now contains tasks!
            )
            
            # Check if tasks already have test cases from unified generation
            # If they do, skip redundant test generation to avoid multiple LLM calls
            tasks_with_tests = [task for task in all_tasks if hasattr(task, 'test_cases') and task.test_cases]
            tasks_without_tests = [task for task in all_tasks if not (hasattr(task, 'test_cases') and task.test_cases)]
            
            if tasks_without_tests:
                # Only generate tests for tasks that don't already have them
                logger.info(f"Generating enhanced test cases for {len(tasks_without_tests)} tasks (skipping {len(tasks_with_tests)} tasks that already have tests)...")
                task_tests = self.generate_test_cases_for_tasks(
                    tasks=tasks_without_tests,
                    coverage_level=TestCoverageLevel.STANDARD,
                    include_in_task_objects=True,
                    custom_llm_client=custom_llm_client
                )
                logger.info(f"Enhanced test generation completed for {len(tasks_without_tests)} tasks")
            else:
                logger.info(f"All {len(all_tasks)} tasks already have test cases from unified generation - skipping redundant test generation")
            
            # ‚úÖ CRITICAL: Update JIRA with test cases for all created tasks
            if not context.dry_run:
                logger.info("Updating JIRA tickets with test cases...")
                for task in all_tasks:
                    if hasattr(task, 'key') and task.key and hasattr(task, 'test_cases') and task.test_cases:
                        logger.info(f"Updating JIRA task {task.key} with {len(task.test_cases)} test cases")
                        try:
                            self._update_ticket_with_test_cases(task.key, task.test_cases)
                        except Exception as e:
                            logger.error(f"Failed to update JIRA task {task.key} with test cases: {str(e)}")
            
            execution_time = time.time() - start_time
            
            # Capture system prompt from LLM client
            llm_to_use = custom_llm_client if custom_llm_client else self.llm_client
            system_prompt = llm_to_use.get_system_prompt() if llm_to_use else None
            
            return PlanningResult(
                epic_key=context.epic_key,
                mode=context.mode,
                success=True,
                created_tickets=created_tickets,
                epic_plan=epic_plan,
                summary_stats={"total_tasks": len(all_tasks)},
                execution_time_seconds=execution_time,
                system_prompt=system_prompt,
                user_prompt="[Multiple prompts used for different stories - see task details]"
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Error generating tasks for stories: {str(e)}")
            
            return PlanningResult(
                epic_key=context.epic_key,
                mode=context.mode,
                success=False,
                errors=[str(e)],
                execution_time_seconds=execution_time
            )
    
    def _generate_epic_plan(self, context: PlanningContext) -> EpicPlan:
        """Generate complete epic plan with stories and tasks"""
        epic_key = context.epic_key
        gap_analysis = context.gap_analysis
        
        # Get epic details
        epic_issue = self.jira_client.get_ticket(epic_key)
        if not epic_issue:
            raise ValueError(f"Epic {epic_key} not found")
        epic_title = epic_issue.get('fields', {}).get('summary', '')
        
        # Handle structured description (Atlassian Document Format)
        epic_description_field = epic_issue.get('fields', {}).get('description', '')
        if isinstance(epic_description_field, dict):
            epic_description = self._extract_text_from_adf(epic_description_field)
        else:
            epic_description = epic_description_field or ''
        
        # Get PRD/RFC content
        prd_content = self._get_prd_content(epic_issue)
        rfc_content = self._get_rfc_content(epic_issue)
        
        # Generate stories (including missing ones)
        stories = self._generate_stories_for_epic_plan(epic_key, epic_description, gap_analysis, prd_content, rfc_content)
        
        # Generate tasks for each story
        for story in stories:
            story.tasks = self._generate_tasks_for_story_plan(story, context)
        
        # Calculate total estimated days
        total_days = sum(
            task.cycle_time_estimate.total_days 
            for story in stories 
            for task in story.tasks 
            if task.cycle_time_estimate
        )
        
        return EpicPlan(
            epic_key=epic_key,
            epic_title=epic_title,
            epic_description=epic_description,
            prd_url=self._get_custom_field_value(epic_issue, 'PRD'),
            rfc_url=self._get_custom_field_value(epic_issue, 'RFC'),
            stories=stories,
            total_estimated_days=total_days
        )
    
    def _generate_stories_for_epic_plan(
        self,
        epic_key: str,
        epic_description: str,
        gap_analysis: GapAnalysis,
        prd_content: Optional[Dict[str, Any]] = None,
        rfc_content: Optional[Dict[str, Any]] = None
    ) -> List[StoryPlan]:
        """Generate stories for an epic"""
        stories = []
        
        # Generate stories for missing areas
        for missing_area in gap_analysis.missing_stories:
            story = self._generate_story_from_area(missing_area, epic_key, prd_content, rfc_content)
            stories.append(story)
        
        return stories
    
    def _generate_story_from_area(
        self,
        area: str,
        epic_key: str,
        prd_content: Optional[Dict[str, Any]] = None,
        rfc_content: Optional[Dict[str, Any]] = None
    ) -> StoryPlan:
        """Generate a story for a specific functional area using enhanced AI prompting"""
        logger.info(f"Generating story for area: {area}")
        
        try:
            # Use enhanced prompt engine for story generation
            epic_context = f"Epic: {epic_key} - Area: {area}"
            doc_type = DocumentType.HYBRID
            
            if prd_content and not rfc_content:
                doc_type = DocumentType.PRD
            elif rfc_content and not prd_content:
                doc_type = DocumentType.RFC
            
            # Generate optimized prompt
            prompt = self.prompt_engine.generate_story_creation_prompt(
                epic_context=epic_context,
                missing_areas=[area],
                doc_type=doc_type
            )
            
            # Add context from documents
            if prd_content:
                prompt += f"\n\nPRD Content for context:\n{str(prd_content)[:1000]}"
            if rfc_content:
                prompt += f"\n\nRFC Content for context:\n{str(rfc_content)[:1000]}"
            
            # Generate story using LLM with centralized system prompt
            # max_tokens=None uses config default (from LLM_MAX_TOKENS env var)
            response = self.llm_client.generate_content(
                prompt=prompt,
                system_prompt=Prompts.get_story_generation_prompt(),
                max_tokens=None
            )
            
            # Parse the LLM response into structured story
            story = self._parse_story_response(response, area, epic_key)
            if story:
                logger.info(f"Successfully generated story for {area}: {story.summary}")
                return story
            
        except Exception as e:
            logger.warning(f"LLM story generation failed for {area}: {str(e)}, falling back to template")
        
        # Fallback to template-based generation
        return self._generate_story_from_template(area, epic_key, prd_content, rfc_content)
    
    def _parse_story_response(self, response: str, area: str, epic_key: str) -> Optional[StoryPlan]:
        """Parse LLM response into StoryPlan structure"""
        try:
            # This would be enhanced with more sophisticated parsing
            # For now, create a basic story structure
            
            # Extract key information from response
            lines = response.strip().split('\n')
            summary = area
            description = f"Generated story for {area}"
            
            # Look for story patterns in response
            for line in lines:
                if line.strip().startswith('Summary:') or line.strip().startswith('Title:'):
                    summary = line.split(':', 1)[1].strip()
                elif line.strip().startswith('Description:') or line.strip().startswith('As a'):
                    description = line.strip()
                    break
            
            # Create default acceptance criteria
            criteria = [
                AcceptanceCriteria(
                    scenario=f"{area} functionality",
                    given="the system is properly configured",
                    when=f"a user interacts with {area.lower()}",
                    then="the expected functionality works correctly"
                )
            ]
            
            # Create test cases
            test_cases = [
                TestCase(
                    title=f"Test {area} functionality",
                    type="integration",
                    description=f"Verify {area} works as expected",
                    expected_result="Feature functions correctly"
                )
            ]
            
            # Create cycle time estimate
            cycle_estimate = CycleTimeEstimate(
                development_days=3.0,
                testing_days=1.0,
                review_days=0.5,
                deployment_days=0.5,
                total_days=5.0,
                confidence_level=0.7
            )
            
            return StoryPlan(
                summary=summary,
                description=description,
                story_points=5,
                acceptance_criteria=criteria,
                test_cases=test_cases,
                tasks=[],  # Will be populated later
                cycle_time_estimate=cycle_estimate,
                epic_key=epic_key
            )
            
        except Exception as e:
            logger.error(f"Error parsing story response: {str(e)}")
            return None
    
    def _generate_story_from_template(
        self,
        area: str,
        epic_key: str,
        prd_content: Optional[Dict[str, Any]] = None,
        rfc_content: Optional[Dict[str, Any]] = None
    ) -> StoryPlan:
        """Generate a story using templates (fallback method)"""
        story_templates = {
            "User Authentication": {
                "summary": "Implement user authentication system",
                "description": "As a user, I need to be able to securely log in and out of the system so that my account and data are protected.",
                "acceptance_criteria": [
                    AcceptanceCriteria(
                        scenario="User login with valid credentials",
                        given="a user has valid username and password",
                        when="they attempt to log in",
                        then="they should be successfully authenticated and redirected to the dashboard"
                    ),
                    AcceptanceCriteria(
                        scenario="User login with invalid credentials",
                        given="a user has invalid username or password",
                        when="they attempt to log in",
                        then="they should see an error message and remain on the login page"
                    )
                ]
            },
            "API Integration": {
                "summary": "Implement API integration layer",
                "description": "As a developer, I need a robust API integration layer so that the system can communicate with external services reliably.",
                "acceptance_criteria": [
                    AcceptanceCriteria(
                        scenario="Successful API call",
                        given="the external API is available",
                        when="a request is made to the integration layer",
                        then="the response should be properly formatted and returned"
                    )
                ]
            }
        }
        
        template = story_templates.get(area, {
            "summary": f"Implement {area}",
            "description": f"As a user, I need {area.lower()} functionality implemented.",
            "acceptance_criteria": [
                AcceptanceCriteria(
                    scenario=f"{area} functionality works",
                    given="the system is running",
                    when=f"I use the {area.lower()} feature",
                    then="it should work as expected"
                )
            ]
        })
        
        # Generate test cases
        test_cases = [
            TestCase(
                title=f"Test {area} functionality",
                description=f"Verify that {area.lower()} works correctly",
                expected_result=f"{area} functions as designed"
            )
        ]
        
        return StoryPlan(
            summary=template["summary"],
            description=template["description"],
            acceptance_criteria=template["acceptance_criteria"],
            test_cases=test_cases,
            epic_key=epic_key
        )
    
    def _generate_tasks_for_story_plan(self, story: StoryPlan, context: PlanningContext, custom_llm_client: Optional['LLMClient'] = None) -> List[TaskPlan]:
        """Generate tasks with embedded test cases using unified generation approach"""
        logger.info(f"Generating unified tasks with tests for story: {story.summary}")
        
        try:
            # Create a custom team task generator if custom LLM client is provided
            if custom_llm_client:
                from .team_based_task_generator import TeamBasedTaskGenerator
                custom_team_generator = TeamBasedTaskGenerator(custom_llm_client, self.prompt_engine)
                task_generator = custom_team_generator
                logger.info("Using custom LLM client for unified task+test generation")
            else:
                task_generator = self.team_task_generator
            
            # ‚úÖ Use unified generation: tasks WITH embedded test cases
            tasks = task_generator.generate_team_separated_tasks_with_tests(
                story=story,
                max_cycle_days=context.max_task_cycle_days,
                test_coverage_level=TestCoverageLevel.STANDARD,  # Could be configurable via context
                force_separation=True,
                prd_content=context.prd_content,
                rfc_content=context.rfc_content,
                additional_context=context.additional_context
            )
            
            # Limit number of tasks per story
            limited_tasks = tasks[:context.max_tasks_per_story]
            
            # Count test cases for logging
            total_tests = sum(len(task.test_cases) for task in limited_tasks)
            
            logger.info(f"Generated {len(limited_tasks)} unified tasks with {total_tests} embedded tests for story {story.summary}: "
                       f"Backend: {len([t for t in limited_tasks if t.team == TaskTeam.BACKEND])}, "
                       f"Frontend: {len([t for t in limited_tasks if t.team == TaskTeam.FRONTEND])}, "
                       f"QA: {len([t for t in limited_tasks if t.team == TaskTeam.QA])}")
            
            return limited_tasks
            
        except Exception as e:
            logger.error(f"Unified task+test generation failed for story {story.summary}: {str(e)}, using fallback")
            # Fallback to separate generation
            return self._generate_fallback_tasks_with_tests(story, context, custom_llm_client)
    
    def _generate_fallback_tasks_with_tests(self, story: StoryPlan, context: PlanningContext, custom_llm_client: Optional['LLMClient'] = None) -> List[TaskPlan]:
        """Fallback: Generate tasks first, then add test cases separately"""
        logger.info(f"Using fallback separate generation for story: {story.summary}")
        
        try:
            # Create task generator
            if custom_llm_client:
                from .team_based_task_generator import TeamBasedTaskGenerator
                task_generator = TeamBasedTaskGenerator(custom_llm_client, self.prompt_engine)
            else:
                task_generator = self.team_task_generator
            
            # Generate tasks using existing method
            tasks = task_generator.generate_team_separated_tasks(
                story=story,
                max_cycle_days=context.max_task_cycle_days,
                force_separation=True,
                prd_content=context.prd_content,
                rfc_content=context.rfc_content,
                additional_context=context.additional_context
            )
            
            # Add test cases separately using enhanced test generator
            for task in tasks:
                try:
                    task.test_cases = self.test_generator.generate_task_test_cases(
                        task=task,
                        coverage_level=TestCoverageLevel.STANDARD,
                        technical_context=None
                    )
                    logger.info(f"Added {len(task.test_cases)} test cases to task: {task.summary}")
                except Exception as test_e:
                    logger.warning(f"Failed to generate test cases for task {task.summary}: {test_e}")
                    # Use minimal fallback tests
                    task.test_cases = [
                        TestCase(
                            title=f"Basic Test: {task.summary}",
                            type="unit",
                            description="Basic functionality test",
                            expected_result="Basic functionality works",
                            source="minimal_fallback"
                        )
                    ]
            
            # Limit tasks
            limited_tasks = tasks[:context.max_tasks_per_story]
            total_tests = sum(len(task.test_cases) for task in limited_tasks)
            
            logger.info(f"Fallback generation completed: {len(limited_tasks)} tasks with {total_tests} test cases")
            return limited_tasks
            
        except Exception as e:
            logger.error(f"Fallback generation also failed: {str(e)}, using basic fallback")
            return self._generate_fallback_team_tasks(story, context)
    
    def _generate_fallback_team_tasks(self, story: StoryPlan, context: PlanningContext) -> List[TaskPlan]:
        """Generate basic team-separated tasks as fallback when AI generation fails"""
        tasks = []
        
        import uuid
        # Backend task
        backend_task = TaskPlan(
            task_id=str(uuid.uuid4()),  # Generate stable identifier for dependency resolution
            summary=f"Backend implementation for {story.summary}",
            purpose="Implement backend logic and data handling",
            scopes=[TaskScope(
                description="Develop backend services, APIs, and data layer",
                complexity="medium",
                dependencies=[],
                deliverable="Working backend implementation"
            )],
            expected_outcomes=["Backend services implemented", "APIs functional", "Data layer working"],
            team=TaskTeam.BACKEND,
            test_cases=[
                TestCase(
                    title=f"Backend unit tests for {story.summary}",
                    type="unit",
                    description="Test backend components and business logic",
                    expected_result="All backend unit tests pass"
                ),
                TestCase(
                    title=f"API integration tests for {story.summary}",
                    type="integration",
                    description="Test API endpoints and data flow",
                    expected_result="API integration tests pass"
                )
            ],
            cycle_time_estimate=CycleTimeEstimate(
                development_days=2.0,
                testing_days=0.5,
                review_days=0.5,
                deployment_days=0.5,
                total_days=3.0,
                confidence_level=0.7,
                exceeds_limit=False
            ),
            epic_key=story.epic_key,
            story_key=getattr(story, 'key', None)
        )
        tasks.append(backend_task)
        
        # Frontend task
        frontend_task = TaskPlan(
            task_id=str(uuid.uuid4()),  # Generate stable identifier for dependency resolution
            summary=f"Frontend implementation for {story.summary}",
            purpose="Implement user interface and user experience",
            scopes=[TaskScope(
                description="Develop UI components, user interactions, and client-side logic",
                complexity="medium",
                dependencies=["Backend implementation"],
                deliverable="Working frontend implementation"
            )],
            expected_outcomes=["UI components implemented", "User interactions functional", "Frontend integrated with backend"],
            team=TaskTeam.FRONTEND,
            test_cases=[
                TestCase(
                    title=f"UI component tests for {story.summary}",
                    type="unit",
                    description="Test UI components and user interface elements",
                    expected_result="UI component tests pass"
                ),
                TestCase(
                    title=f"User interaction tests for {story.summary}",
                    type="e2e",
                    description="Test end-to-end user interactions and workflows",
                    expected_result="User interaction tests pass"
                )
            ],
            cycle_time_estimate=CycleTimeEstimate(
                development_days=1.5,
                testing_days=0.5,
                review_days=0.5,
                deployment_days=0.25,
                total_days=2.75,
                confidence_level=0.7,
                exceeds_limit=False
            ),
            epic_key=story.epic_key,
            story_key=getattr(story, 'key', None)
        )
        tasks.append(frontend_task)
        
        # QA task
        qa_task = TaskPlan(
            task_id=str(uuid.uuid4()),  # Generate stable identifier for dependency resolution
            summary=f"Quality assurance for {story.summary}",
            purpose="Ensure quality and validate functionality",
            scopes=[TaskScope(
                description="Create comprehensive test plan, execute test cases, and validate quality",
                complexity="medium",
                dependencies=["Frontend implementation"],
                deliverable="Quality validation and test results"
            )],
            expected_outcomes=["Test plan created", "All test cases executed", "Quality validated", "Bugs identified and tracked"],
            team=TaskTeam.QA,
            test_cases=[
                TestCase(
                    title=f"Test plan creation for {story.summary}",
                    type="acceptance",
                    description="Create comprehensive test plan covering all acceptance criteria",
                    expected_result="Complete test plan documented"
                ),
                TestCase(
                    title=f"End-to-end testing for {story.summary}",
                    type="e2e",
                    description="Execute complete end-to-end testing of the feature",
                    expected_result="All E2E tests pass successfully"
                )
            ],
            cycle_time_estimate=CycleTimeEstimate(
                development_days=0.5,
                testing_days=2.0,
                review_days=0.25,
                deployment_days=0.25,
                total_days=3.0,
                confidence_level=0.8,
                exceeds_limit=False
            ),
            epic_key=story.epic_key,
            story_key=getattr(story, 'key', None)
        )
        tasks.append(qa_task)
        
        # ‚úÖ ADD: Generate enhanced test cases for fallback tasks too
        try:
            for task in tasks:
                enhanced_test_cases = self.test_generator.generate_task_test_cases(
                    task=task,
                    coverage_level=TestCoverageLevel.BASIC,  # Use basic level for fallback
                    technical_context=self._detect_technical_context(task)
                )
                # Replace the manually created test cases with enhanced ones
                if enhanced_test_cases:
                    task.test_cases = enhanced_test_cases
                    logger.info(f"Generated {len(enhanced_test_cases)} enhanced test cases for fallback task: {task.summary}")
        except Exception as e:
            logger.warning(f"Enhanced test generation failed for fallback tasks: {str(e)}")
            # Keep the manually created test cases as final fallback
        
        return tasks[:context.max_tasks_per_story]
    
    def _generate_tasks_for_story(self, story_key: str, context: PlanningContext, custom_llm_client: Optional['LLMClient'] = None) -> List[TaskPlan]:
        """Generate tasks for an existing story"""
        try:
            # Get story details
            story_issue = self.jira_client.get_ticket(story_key)
            if not story_issue:
                logger.error(f"Story {story_key} not found")
                return []
            story_summary = story_issue.get('fields', {}).get('summary', '')
            
            # Create a minimal story plan for task generation
            description_field = story_issue.get('fields', {}).get('description', '')
            # Handle structured description (Atlassian Document Format)
            if isinstance(description_field, dict):
                # Extract text from ADF structure
                description = self._extract_text_from_adf(description_field)
            else:
                description = description_field or ''
                
            story_plan = StoryPlan(
                key=story_key,  # ‚úÖ Set the story key!
                summary=story_summary,
                description=description,
                acceptance_criteria=[],
                epic_key=context.epic_key
            )
            
            logger.debug(f"Created StoryPlan with key='{story_plan.key}' for story {story_key}")
            
            generated_tasks = self._generate_tasks_for_story_plan(story_plan, context, custom_llm_client)
            
            # Verify story_key is set in generated tasks
            for task in generated_tasks:
                logger.debug(f"Generated task '{task.summary}' with story_key='{task.story_key}'")
                if not task.story_key:
                    logger.warning(f"‚ö†Ô∏è Task '{task.summary}' missing story_key, setting manually")
                    task.story_key = story_key
            
            return generated_tasks
            
        except Exception as e:
            logger.error(f"Error generating tasks for story {story_key}: {str(e)}")
            return []
    
    def _get_prd_content(self, epic_issue) -> Optional[Dict[str, Any]]:
        """Extract PRD content from epic using enhanced section extraction"""
        try:
            prd_url = self._get_custom_field_value(epic_issue, 'PRD')
            if not prd_url:
                return None
            
            # Use the correct confluence client method to get enhanced PRD content
            page_data = self.confluence_client.get_page_content(prd_url)
            if not page_data:
                return None
            
            # Extract enhanced PRD sections for planning context
            prd_sections = page_data.get('prd_sections', {})
            
            # Return structured PRD data for planning
            return {
                'title': page_data['title'],
                'url': page_data['url'],
                'summary': page_data.get('summary'),
                'goals': page_data.get('goals'),
                'content': page_data.get('content', ''),
                'sections': prd_sections
            }
            
        except Exception as e:
            logger.warning(f"Failed to get PRD content from epic: {e}")
            return None
    
    def _get_rfc_content(self, epic_issue) -> Optional[Dict[str, Any]]:
        """Extract RFC content from epic using enhanced section extraction"""
        try:
            rfc_url = self._get_custom_field_value(epic_issue, 'RFC')
            if not rfc_url:
                return None
            
            # Use the correct confluence client method to get enhanced RFC content
            page_data = self.confluence_client.get_page_content(rfc_url)
            if not page_data:
                return None
            
            # Extract enhanced RFC sections for planning context
            rfc_sections = page_data.get('rfc_sections', {})
            
            # Return structured RFC data for planning
            return {
                'title': page_data['title'],
                'url': page_data['url'],
                'summary': page_data.get('summary'),
                'content': page_data.get('content', ''),
                'sections': rfc_sections
            }
            
        except Exception as e:
            logger.warning(f"Failed to get RFC content from epic: {e}")
            return None
    
    def _get_custom_field_value(self, issue, field_type: str) -> Optional[str]:
        """Get custom field value for PRD or RFC"""
        try:
            if field_type == 'PRD':
                field_id = self.jira_client.prd_custom_field
            elif field_type == 'RFC':
                field_id = self.jira_client.rfc_custom_field
            else:
                return None
            
            if not field_id:
                return None
                
            return issue.get('fields', {}).get(field_id, None)
        except Exception:
            return None
    
    def _create_epic_tickets(self, epic_plan: EpicPlan) -> Dict[str, List[str]]:
        """Create all tickets for an epic plan"""
        created_tickets = {"stories": [], "tasks": []}
        
        # Create stories
        for story in epic_plan.stories:
            story_key = self._create_story_ticket(story)
            if story_key:
                created_tickets["stories"].append(story_key)
                
                # Create tasks for this story
                for task in story.tasks:
                    task.story_key = story_key
                    task_key = self._create_task_ticket(task)
                    if task_key:
                        created_tickets["tasks"].append(task_key)
        
        return created_tickets
    
    def _create_story_tickets(self, stories: List[StoryPlan]) -> Dict[str, List[str]]:
        """Create story tickets"""
        created_tickets = {"stories": []}
        
        for story in stories:
            story_key = self._create_story_ticket(story)
            if story_key:
                created_tickets["stories"].append(story_key)
        
        return created_tickets
    
    def _create_task_tickets(self, tasks: List[TaskPlan]) -> List[str]:
        """Create task tickets and establish relationships"""
        logger.info(f"Creating {len(tasks)} task tickets in JIRA...")
        created_keys = []
        failed_count = 0
        
        # Build a mapping of task summaries to tasks for dependency resolution
        self._task_summary_to_plan = {task.summary: task for task in tasks}
        self._task_summary_to_key = {}
        
        # Build task_id to key mapping for stable dependency resolution
        self._task_id_to_key = {}
        self._task_id_to_plan = {task.task_id: task for task in tasks if task.task_id}
        
        # Build normalized mappings for faster dependency lookup (backward compatibility)
        self._normalized_summary_to_key = {}
        self._normalized_summary_to_original = {}
        
        # First pass: Create all tickets
        for i, task in enumerate(tasks, 1):
            logger.debug(f"Creating task {i}/{len(tasks)}: {task.summary} (task_id: {task.task_id})")
            task_key = self._create_task_ticket_only(task)
            if task_key:
                created_keys.append(task_key)
                # Store original summary mapping (backward compatibility)
                self._task_summary_to_key[task.summary] = task_key
                
                # Store task_id mapping (preferred for dependency resolution)
                if task.task_id:
                    self._task_id_to_key[task.task_id] = task_key
                    logger.debug(f"   Mapped task_id {task.task_id} -> {task_key}")
                
                # Store normalized summary mapping for faster lookup (backward compatibility)
                normalized_summary = self._normalize_task_summary(task.summary)
                if normalized_summary:
                    # If multiple tasks have same normalized summary, keep the first one
                    if normalized_summary not in self._normalized_summary_to_key:
                        self._normalized_summary_to_key[normalized_summary] = task_key
                        self._normalized_summary_to_original[normalized_summary] = task.summary
                    else:
                        logger.debug(f"   Note: Normalized summary '{normalized_summary}' already mapped to another task")
                
                logger.debug(f"Task {i} created successfully: {task_key}")
            else:
                failed_count += 1
                logger.warning(f"Task {i} creation failed: {task.summary}")
        
        logger.debug(f"Built dependency mappings: {len(self._task_summary_to_key)} summaries, {len(self._task_id_to_key)} task_ids, {len(self._normalized_summary_to_key)} normalized")
        
        # Second pass: Create relationships after all tickets exist
        logger.info(f"Creating relationships for {len(created_keys)} created tasks...")
        for task in tasks:
            if task.key:  # Only process successfully created tasks
                self._create_task_relationships(task, task.key)
        
        logger.info(f"Task creation completed: {len(created_keys)} succeeded, {failed_count} failed")
        return created_keys
    
    def _create_story_ticket(self, story: StoryPlan) -> Optional[str]:
        """Create a single story ticket"""
        try:
            logger.info(f"Creating JIRA story ticket: {story.summary}")
            logger.debug(f"Story details - Epic: {story.epic_key}, Priority: {story.priority}")
            
            # Use the project key from the story's epic
            project_key = None
            if story.epic_key:
                # Extract project key from epic key (e.g., "PROJ-5840" -> "PROJ")
                project_key = story.epic_key.split('-')[0]
            
            if not project_key:
                logger.error(f"Cannot determine project key for story: {story.summary}")
                return None
            
            logger.debug(f"Using project key: {project_key}")
            
            # Create the ticket using JIRA client
            # Pass Confluence server URL for image attachment support
            confluence_server_url = None
            if self.confluence_client:
                confluence_server_url = self.confluence_client.server_url
            
            story_key = self.jira_client.create_story_ticket(
                story_plan=story,
                project_key=project_key,
                confluence_server_url=confluence_server_url
            )
            
            if story_key:
                logger.info(f"‚úÖ Successfully created story ticket: {story_key}")
                # Update the story with the created key
                story.key = story_key
                
                # ‚ùå SKIP: Don't write generic test cases during story creation  
                # Let enhanced test generation write proper test cases instead
                if hasattr(story, 'test_cases') and story.test_cases:
                    logger.info(f"Skipping generic test cases during story creation for {story_key} - enhanced test generation will provide better ones")
            else:
                logger.error(f"‚ùå Failed to create story ticket for: {story.summary}")
            
            return story_key
            
        except Exception as e:
            logger.error(f"‚ùå Error creating story ticket for '{story.summary}': {str(e)}")
            logger.exception("Full exception details:")
            return None
    
    def _create_task_ticket_only(self, task: TaskPlan) -> Optional[str]:
        """Create a single task ticket without relationships"""
        try:
            logger.info(f"Creating JIRA task ticket: {task.summary}")
            logger.debug(f"Task details - Team: {task.team}, Epic: {task.epic_key}, Story: {getattr(task, 'story_key', 'None')}")
            
            # Use the project key from the task's epic or story
            project_key = None
            if task.epic_key:
                # Extract project key from epic key (e.g., "PROJ-5840" -> "PROJ")
                project_key = task.epic_key.split('-')[0]
            elif hasattr(task, 'story_key') and task.story_key:
                # Extract project key from story key
                project_key = task.story_key.split('-')[0]
            
            if not project_key:
                logger.error(f"Cannot determine project key for task: {task.summary}")
                return None
            
            logger.debug(f"Using project key: {project_key}")
            
            # Create the ticket using JIRA client
            # Pass Confluence server URL for image attachment support
            confluence_server_url = None
            if self.confluence_client:
                confluence_server_url = self.confluence_client.server_url
            
            task_key = self.jira_client.create_task_ticket(
                task_plan=task,
                project_key=project_key,
                story_key=getattr(task, 'story_key', None),
                confluence_server_url=confluence_server_url
            )
            
            if task_key:
                logger.info(f"‚úÖ Successfully created task ticket: {task_key}")
                # Update the task with the created key
                task.key = task_key
                
                # ‚ùå SKIP: Don't write generic test cases during task creation
                # Let enhanced test generation write proper test cases instead
                if hasattr(task, 'test_cases') and task.test_cases:
                    logger.info(f"Skipping generic test cases during task creation for {task_key} - enhanced test generation will provide better ones")
            else:
                logger.error(f"‚ùå Failed to create task ticket for: {task.summary}")
            
            return task_key
            
        except Exception as e:
            logger.error(f"‚ùå Error creating task ticket for '{task.summary}': {str(e)}")
            logger.exception("Full exception details:")
            return None

    def _create_task_relationships(self, task: TaskPlan, task_key: str):
        """Create issue links for task relationships"""
        try:
            logger.debug(f"üìã Creating relationships for task {task_key}")
            logger.debug(f"   Task details: story_key='{task.story_key}', epic_key='{task.epic_key}', team={task.team.value}")
            
            relationships_created = 0
            
            # Handle "depends_on_tasks" relationships - create "Blocks" links
            if task.depends_on_tasks:
                logger.info(f"üîó Processing {len(task.depends_on_tasks)} dependency(ies) for task {task_key}: {task.depends_on_tasks}")
                dependency_resolved = 0
                dependency_failed = 0
                
                for dependency_identifier in task.depends_on_tasks:
                    logger.debug(f"   Resolving dependency: '{dependency_identifier}'")
                    # Try to find the actual JIRA key for the dependency task
                    # First try task_id lookup (preferred), then fall back to summary lookup (backward compatibility)
                    dependency_key = self._find_task_key_by_identifier(dependency_identifier)
                    if dependency_key:
                        # Create "Blocks" link: dependency blocks this task
                        # Try "Blocks" first, fallback to standard link types
                        success = self._create_dependency_link(dependency_key, task_key)
                        if success:
                            logger.info(f"‚úÖ Created dependency link: {dependency_key} blocks {task_key} (dependency: '{dependency_identifier}')")
                            relationships_created += 1
                            dependency_resolved += 1
                        else:
                            logger.warning(f"‚ùå Failed to create dependency link: {dependency_key} -> {task_key} (dependency: '{dependency_identifier}')")
                            dependency_failed += 1
                    else:
                        logger.warning(f"üîç Could not resolve dependency task: '{dependency_identifier}' (task {task_key} depends on this)")
                        dependency_failed += 1
                
                # Summary logging
                if dependency_resolved > 0:
                    logger.info(f"   ‚úÖ Resolved {dependency_resolved}/{len(task.depends_on_tasks)} dependencies for {task_key}")
                if dependency_failed > 0:
                    logger.warning(f"   ‚ö†Ô∏è Failed to resolve {dependency_failed}/{len(task.depends_on_tasks)} dependencies for {task_key}")
            
            # Handle story relationship - create "Split from" link only to stories
            if task.story_key:
                logger.info(f"Creating story-task link: {task_key} split from story {task.story_key}")
                success, actual_link_type = self._create_parent_child_link(task.story_key, task_key)
                if success:
                    logger.info(f"‚úÖ Created '{actual_link_type}' link: {task_key} linked to story {task.story_key}")
                    relationships_created += 1
                else:
                    logger.warning(f"‚ùå Failed to create any relationship link: story {task.story_key} -> {task_key}")
            
            # For tasks without story parent, JIRA client already created native parent relationship to epic
            elif task.epic_key:
                logger.info(f"‚ÑπÔ∏è Task {task_key} uses native JIRA parent relationship to epic {task.epic_key} (no custom link needed)")
            
            # Log when no parent relationship is available
            else:
                logger.warning(f"‚ö†Ô∏è Task {task_key} has no story_key or epic_key for parent relationship")
            
            # Log team blocking information (informational only, as we can't create links to teams)
            if task.blocked_by_teams:
                logger.info(f"üö´ Task {task_key} is blocked by teams: {[team.value for team in task.blocked_by_teams]}")
                logger.debug(f"   ‚ÑπÔ∏è Team blocking info is included in task description, no JIRA links created")
            
            logger.debug(f"üìä Relationship creation summary for {task_key}: {relationships_created} custom links created")
            
        except Exception as e:
            logger.error(f"üí• Error creating relationships for task {task_key}: {str(e)}")
            logger.exception("Full exception details:")

    def _normalize_task_summary(self, summary: str) -> str:
        """
        Normalize task summary for matching:
        - Remove team prefixes ([BE], [FE], [QA])
        - Strip whitespace
        - Convert to lowercase
        """
        if not summary:
            return ""
        
        # Remove team prefixes (case-insensitive)
        normalized = summary.strip()
        # Match [BE], [FE], [QA] at the start (with optional whitespace)
        normalized = re.sub(r'^\s*\[(BE|FE|QA)\]\s*', '', normalized, flags=re.IGNORECASE)
        # Also handle without brackets: BE, FE, QA at start
        normalized = re.sub(r'^\s*(BE|FE|QA):\s*', '', normalized, flags=re.IGNORECASE)
        normalized = re.sub(r'^\s*(BE|FE|QA)\s+', '', normalized, flags=re.IGNORECASE)
        
        # Normalize whitespace and case
        normalized = ' '.join(normalized.split())  # Collapse multiple spaces
        normalized = normalized.lower()
        
        return normalized
    
    def _fuzzy_match_dependency(self, dependency_summary: str, available_summaries: List[str]) -> Optional[str]:
        """
        Try to find a fuzzy/partial match for dependency summary in available summaries.
        Returns the first matching summary if found, None otherwise.
        """
        normalized_dep = self._normalize_task_summary(dependency_summary)
        if not normalized_dep:
            return None
        
        # Try different matching strategies
        for summary in available_summaries:
            normalized_summary = self._normalize_task_summary(summary)
            
            # Strategy 1: Exact normalized match
            if normalized_dep == normalized_summary:
                logger.debug(f"Fuzzy match (exact normalized): '{dependency_summary}' -> '{summary}'")
                return summary
            
            # Strategy 2: Dependency is contained in summary (dependency is shorter/more specific)
            if normalized_dep in normalized_summary and len(normalized_dep) >= 10:
                logger.debug(f"Fuzzy match (dependency contained in summary): '{dependency_summary}' -> '{summary}'")
                return summary
            
            # Strategy 3: Summary is contained in dependency (summary is shorter/more specific)
            if normalized_summary in normalized_dep and len(normalized_summary) >= 10:
                logger.debug(f"Fuzzy match (summary contained in dependency): '{dependency_summary}' -> '{summary}'")
                return summary
            
            # Strategy 4: Word overlap (at least 3 significant words match)
            dep_words = set(word for word in normalized_dep.split() if len(word) > 3)
            summary_words = set(word for word in normalized_summary.split() if len(word) > 3)
            if dep_words and summary_words:
                overlap = dep_words.intersection(summary_words)
                # If at least 50% of dependency words match, consider it a match
                if len(overlap) >= min(3, len(dep_words) * 0.5):
                    logger.debug(f"Fuzzy match (word overlap {len(overlap)} words): '{dependency_summary}' -> '{summary}'")
                    return summary
        
        return None

    def _find_task_key_by_identifier(self, identifier: str) -> Optional[str]:
        """
        Find JIRA key for a task by identifier (task_id or summary).
        Prefers task_id lookup, falls back to summary lookup for backward compatibility.
        
        Args:
            identifier: Either a task_id (UUID) or task summary string
            
        Returns:
            JIRA task key if found, None otherwise
        """
        # First try task_id lookup (preferred method)
        if hasattr(self, '_task_id_to_key') and identifier in self._task_id_to_key:
            task_key = self._task_id_to_key[identifier]
            logger.debug(f"‚úÖ Found task by task_id: {identifier} -> {task_key}")
            return task_key
        
        # Fall back to summary-based lookup (backward compatibility)
        return self._find_task_key_by_summary(identifier)
    
    def _find_task_key_by_summary(self, task_summary: str) -> Optional[str]:
        """
        Find JIRA key for a task by its summary with multiple matching strategies:
        1. Exact match
        2. Normalized match (case-insensitive, whitespace-normalized, team prefix removed)
        3. Fuzzy/partial match (containment or word overlap)
        """
        try:
            logger.debug(f"üîç Looking for JIRA key for dependency: '{task_summary}'")
            
            if not hasattr(self, '_task_summary_to_key') or not self._task_summary_to_key:
                logger.warning(f"‚ö†Ô∏è No task summary mapping available for dependency lookup")
                return None
            
            # Strategy 1: Exact match (current behavior)
            if task_summary in self._task_summary_to_key:
                task_key = self._task_summary_to_key[task_summary]
                logger.info(f"‚úÖ Found exact match: '{task_summary}' -> {task_key}")
                return task_key
            
            logger.debug(f"   Exact match failed, trying normalized match...")
            
            # Strategy 2: Normalized match
            normalized_dep = self._normalize_task_summary(task_summary)
            if normalized_dep:
                # Check if we have normalized mapping
                if hasattr(self, '_normalized_summary_to_key'):
                    if normalized_dep in self._normalized_summary_to_key:
                        task_key = self._normalized_summary_to_key[normalized_dep]
                        original_summary = self._normalized_summary_to_original.get(normalized_dep, task_summary)
                        logger.info(f"‚úÖ Found normalized match: '{task_summary}' -> '{original_summary}' -> {task_key}")
                        return task_key
                
                # Fallback: iterate through mapping and compare normalized versions
                for original_summary, task_key in self._task_summary_to_key.items():
                    normalized_original = self._normalize_task_summary(original_summary)
                    if normalized_dep == normalized_original:
                        logger.info(f"‚úÖ Found normalized match (fallback): '{task_summary}' -> '{original_summary}' -> {task_key}")
                        return task_key
            
            logger.debug(f"   Normalized match failed, trying fuzzy match...")
            
            # Strategy 3: Fuzzy/partial match
            available_summaries = list(self._task_summary_to_key.keys())
            matched_summary = self._fuzzy_match_dependency(task_summary, available_summaries)
            if matched_summary:
                task_key = self._task_summary_to_key[matched_summary]
                logger.info(f"‚úÖ Found fuzzy match: '{task_summary}' -> '{matched_summary}' -> {task_key}")
                return task_key
            
            # All strategies failed
            logger.warning(f"‚ùå Could not find JIRA key for dependency: '{task_summary}'")
            logger.debug(f"   Available task summaries in mapping ({len(available_summaries)}):")
            for i, summary in enumerate(available_summaries[:10], 1):  # Show first 10
                logger.debug(f"     {i}. {summary}")
            if len(available_summaries) > 10:
                logger.debug(f"     ... and {len(available_summaries) - 10} more")
            
            return None
            
        except Exception as e:
            logger.error(f"üí• Error finding task key for summary '{task_summary}': {str(e)}")
            logger.exception("Full exception details:")
            return None

    def _create_dependency_link(self, dependency_key: str, task_key: str) -> bool:
        """
        Create a dependency link between tasks with fallback options
        """
        try:
            # Try different link types in order of preference
            link_types_to_try = ["Blocks", "Depends", "Relates"]
            
            for link_type in link_types_to_try:
                logger.debug(f"Attempting to create {link_type} link: {dependency_key} -> {task_key}")
                success = self.jira_client.create_issue_link(
                    inward_key=dependency_key,
                    outward_key=task_key,
                    link_type=link_type
                )
                if success:
                    logger.info(f"‚úÖ Created {link_type} link: {dependency_key} -> {task_key}")
                    return True
                else:
                    logger.warning(f"Failed to create {link_type} link, trying next option...")
            
            logger.error(f"‚ùå Failed to create any dependency link: {dependency_key} -> {task_key}")
            return False
            
        except Exception as e:
            logger.error(f"Error creating dependency link {dependency_key} -> {task_key}: {str(e)}")
            return False

    def _create_parent_child_link(self, parent_key: str, child_key: str) -> tuple[bool, str]:
        """
        Create a parent-child link with fallback options
        
        Returns:
            Tuple of (success, actual_link_type_used)
        """
        try:
            # Try different link types in order of preference
            # Note: "Work item split" is the correct JIRA link type name (not "Split from")
            link_types_to_try = ["Work item split", "Subtask", "Child", "Relates"]
            
            split_from_failed = False
            for i, link_type in enumerate(link_types_to_try):
                logger.debug(f"Attempting to create {link_type} link: {parent_key} -> {child_key}")
                success = self.jira_client.create_issue_link(
                    inward_key=parent_key,
                    outward_key=child_key,
                    link_type=link_type
                )
                if success:
                    logger.info(f"‚úÖ Created {link_type} link: {parent_key} -> {child_key}")
                    return True, link_type
                else:
                    if link_type == "Work item split":
                        split_from_failed = True
                        logger.warning(f"üö® Work item split link failed - checking available link types...")
                        # Get available link types for debugging
                        available_types = self.jira_client.get_available_link_types()
                        if available_types:
                            logger.info(f"üîç Available link types check completed (see above)")
                        else:
                            logger.warning(f"‚ö†Ô∏è Could not retrieve available link types")
                    logger.warning(f"Failed to create {link_type} link, trying next option...")
            
            if split_from_failed:
                logger.error(f"üí° RECOMMENDATION: 'Work item split' link type failed.")
                logger.error(f"üí° Check JIRA Admin > System > Issue linking configuration")
            
            logger.error(f"‚ùå Failed to create any parent-child link: {parent_key} -> {child_key}")
            return False, "none"
            
        except Exception as e:
            logger.error(f"Error creating parent-child link {parent_key} -> {child_key}: {str(e)}")
            return False

    def _update_ticket_with_test_cases(self, ticket_key: str, test_cases: List[TestCase]):
        """Update JIRA ticket with test cases in description or custom field"""
        try:
            if not test_cases:
                return
            
            # Format test cases as plain text with clean Gherkin format
            test_cases_text = "TEST CASES\n" + "=" * 60 + "\n\n"
            
            for test_case in test_cases:
                test_cases_text += f"{test_case.title}\n"
                test_cases_text += "-" * len(test_case.title) + "\n\n"
                
                test_cases_text += f"Type: {test_case.type}\n\n"
                test_cases_text += f"Description:\n{test_case.description}\n\n"
                
                # Handle both 'steps' (legacy) and 'test_steps' (enhanced) fields
                steps_content = None
                if hasattr(test_case, 'test_steps') and test_case.test_steps:
                    steps_content = test_case.test_steps
                elif hasattr(test_case, 'steps') and test_case.steps:
                    steps_content = test_case.steps
                
                if steps_content:
                    test_cases_text += f"Test Steps:\n"
                    if isinstance(steps_content, list):
                        # Join list items as plain lines without numbering
                        for step in steps_content:
                            test_cases_text += f"{step}\n"
                    else:
                        # Format Gherkin steps without numbering - preserve clean format
                        lines = steps_content.split('\n')
                        for line in lines:
                            line = line.strip()
                            if line:
                                test_cases_text += f"{line}\n"
                    test_cases_text += "\n"
                
                test_cases_text += f"Expected Result:\n{test_case.expected_result}\n\n"
                
                if hasattr(test_case, 'priority') and test_case.priority:
                    test_cases_text += f"Priority: {test_case.priority}\n\n"
                
                if hasattr(test_case, 'source') and test_case.source:
                    test_cases_text += f"Source: {test_case.source}\n\n"
                
                test_cases_text += "=" * 60 + "\n\n"
            
            # Update the ticket using custom field (with fallback to description)
            success = self.jira_client.update_test_case_custom_field(ticket_key, test_cases_text)
            
            if success:
                logger.info(f"‚úÖ Successfully updated {ticket_key} with test cases in custom field")
            else:
                logger.warning(f"‚ö†Ô∏è Failed to update {ticket_key} with test cases")
                
        except Exception as e:
            logger.error(f"Error updating ticket {ticket_key} with test cases: {str(e)}")

    def verify_test_case_integration(self, ticket_key: str) -> Dict[str, Any]:
        """Verify that test cases were properly added to a JIRA ticket"""
        try:
            ticket = self.jira_client.get_ticket(ticket_key)
            if not ticket:
                return {"success": False, "error": "Ticket not found"}
            
            fields = ticket.get('fields', {})
            
            # Check custom field first (preferred method)
            test_case_field_content = ""
            has_test_cases_in_custom_field = False
            
            if self.jira_client.test_case_custom_field:
                test_case_field_content = fields.get(self.jira_client.test_case_custom_field, '')
                if test_case_field_content:
                    has_test_cases_in_custom_field = "## Test Cases" in str(test_case_field_content)
            
            # Check description as fallback
            description = fields.get('description', '')
            description_text = ""
            has_test_cases_in_description = False
            
            if isinstance(description, dict):
                description_text = self._extract_text_from_adf(description)
            else:
                description_text = str(description) if description else ""
            
            has_test_cases_in_description = "## Test Cases" in description_text
            
            # Overall test case presence
            has_test_cases = has_test_cases_in_custom_field or has_test_cases_in_description
            
            return {
                "success": True,
                "ticket_key": ticket_key,
                "has_test_cases": has_test_cases,
                "test_cases_location": {
                    "custom_field": has_test_cases_in_custom_field,
                    "description": has_test_cases_in_description,
                    "custom_field_id": self.jira_client.test_case_custom_field
                },
                "custom_field_content_length": len(str(test_case_field_content)),
                "description_length": len(description_text),
                "custom_field_preview": str(test_case_field_content)[:200] if test_case_field_content else "",
                "description_preview": description_text[:200] if description_text else ""
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}

    # =============================================================================
    # BULK TICKET CREATION METHODS
    # =============================================================================
    
    def create_epic_structure(self, epic_plan: EpicPlan, dry_run: bool = True) -> Dict[str, Any]:
        """
        Create complete epic structure with stories and tasks
        
        Args:
            epic_plan: Complete epic plan with stories and tasks  
            dry_run: If True, simulate creation without actually creating tickets
            
        Returns:
            Creation results with ticket keys and any errors
        """
        logger.info(f"Creating epic structure for {epic_plan.epic_key} (dry_run={dry_run})")
        return self.bulk_creator.create_epic_structure(epic_plan, dry_run)
    
    def create_stories_for_epic(self, epic_key: str, stories: List[StoryPlan], dry_run: bool = True) -> Dict[str, Any]:
        """
        Create stories for an epic using bulk operations
        
        Args:
            epic_key: Parent epic key
            stories: List of story plans to create
            dry_run: If True, simulate creation
            
        Returns:
            Creation results
        """
        logger.info(f"Creating {len(stories)} stories for epic {epic_key} (dry_run={dry_run})")
        return self.bulk_creator.create_stories_only(epic_key, stories, dry_run)
    
    def create_tasks_for_stories(self, tasks: List[TaskPlan], story_keys: List[str], dry_run: bool = True) -> Dict[str, Any]:
        """
        Create tasks for existing stories using bulk operations
        
        Args:
            tasks: List of task plans to create
            story_keys: List of parent story keys
            dry_run: If True, simulate creation
            
        Returns:
            Creation results
        """
        logger.info(f"Creating {len(tasks)} tasks for {len(story_keys)} stories (dry_run={dry_run})")
        return self.bulk_creator.create_tasks_only(tasks, story_keys, dry_run)
    
    def execute_planning_with_creation(self, context: PlanningContext, create_tickets: bool = False) -> Dict[str, Any]:
        """
        Execute complete planning workflow and optionally create tickets
        
        Args:
            context: Planning context with epic key and options
            create_tickets: If True, actually create tickets in JIRA
            
        Returns:
            Combined planning and creation results
        """
        start_time = time.time()
        logger.info(f"Executing planning workflow for {context.epic_key} (create_tickets={create_tickets})")
        
        results = {
            "epic_key": context.epic_key,
            "create_tickets": create_tickets,
            "success": True,
            "planning_results": None,
            "creation_results": None,
            "errors": [],
            "execution_time_seconds": 0.0
        }
        
        try:
            # Step 1: Generate the epic plan
            planning_result = self.plan_epic_complete(context)
            results["planning_results"] = planning_result.dict()
            
            if not planning_result.success:
                results["success"] = False
                results["errors"].extend(planning_result.errors)
                return results
            
            # Step 2: Create tickets if requested
            if create_tickets and planning_result.epic_plan:
                creation_result = self.create_epic_structure(
                    planning_result.epic_plan, 
                    dry_run=not create_tickets
                )
                results["creation_results"] = creation_result
                
                if not creation_result["success"]:
                    results["success"] = False
                    results["errors"].extend(creation_result["errors"])
                    
                    # If creation failed, log rollback information
                    if creation_result.get("created_tickets"):
                        rollback_info = self.bulk_creator.rollback_creation(
                            creation_result["created_tickets"]
                        )
                        results["rollback_info"] = rollback_info
            
        except Exception as e:
            results["success"] = False
            results["errors"].append(f"Unexpected error in planning workflow: {str(e)}")
            logger.error(f"Error in planning workflow: {str(e)}")
        
        finally:
            results["execution_time_seconds"] = time.time() - start_time
            logger.info(f"Planning workflow completed in {results['execution_time_seconds']:.2f}s")
        
        return results
    
    def validate_epic_structure(self, epic_key: str) -> Dict[str, Any]:
        """
        Validate epic structure integrity
        
        Args:
            epic_key: Epic to validate
            
        Returns:
            Validation results
        """
        logger.info(f"Validating epic structure for {epic_key}")
        return self.jira_client.validate_ticket_relationships(epic_key)
    
    def generate_comprehensive_test_suite(self, epic_key: str, coverage_level: TestCoverageLevel = TestCoverageLevel.STANDARD) -> Dict[str, Any]:
        """
        Generate comprehensive test suites for all stories and tasks in an epic
        
        Args:
            epic_key: Epic to generate tests for
            coverage_level: Level of test coverage desired
            
        Returns:
            Test generation results with all test cases
        """
        logger.info(f"Generating comprehensive test suite for epic {epic_key} (coverage: {coverage_level.value})")
        
        start_time = time.time()
        results = {
            "success": False,
            "epic_key": epic_key,
            "coverage_level": coverage_level.value,
            "story_tests": {},
            "task_tests": {},
            "total_test_cases": 0,
            "test_statistics": {},
            "errors": [],
            "execution_time_seconds": 0
        }
        
        try:
            # Get epic structure
            epic_analysis = self.analysis_engine.analyze_epic_completeness(epic_key)
            
            if not epic_analysis["success"]:
                results["errors"].append("Failed to analyze epic structure")
                return results
            
            epic_info = epic_analysis["epic_info"]
            
            # Generate tests for stories
            story_tests = {}
            for story in epic_info.get("stories", []):
                try:
                    story_plan = self._convert_jira_story_to_plan(story)
                    if story_plan:
                        # Determine domain context from epic or story
                        domain_context = self._extract_domain_context(epic_info, story)
                        
                        test_cases = self.test_generator.generate_story_test_cases(
                            story=story_plan,
                            coverage_level=coverage_level,
                            domain_context=domain_context
                        )
                        
                        story_tests[story["key"]] = {
                            "story_summary": story.get("summary", ""),
                            "test_cases": [self._test_case_to_dict(tc) for tc in test_cases],
                            "test_count": len(test_cases),
                            "coverage_level": coverage_level.value
                        }
                        
                        logger.info(f"Generated {len(test_cases)} test cases for story {story['key']}")
                        
                except Exception as e:
                    logger.error(f"Error generating tests for story {story.get('key', 'unknown')}: {str(e)}")
                    results["errors"].append(f"Story {story.get('key', 'unknown')}: {str(e)}")
            
            # Generate tests for tasks
            task_tests = {}
            for story in epic_info.get("stories", []):
                for task in story.get("subtasks", []):
                    try:
                        task_plan = self._convert_jira_task_to_plan(task)
                        if task_plan:
                            # Determine technical context from task
                            technical_context = self._extract_technical_context(task)
                            
                            test_cases = self.test_generator.generate_task_test_cases(
                                task=task_plan,
                                coverage_level=coverage_level,
                                technical_context=technical_context
                            )
                            
                            task_tests[task["key"]] = {
                                "task_summary": task.get("summary", ""),
                                "parent_story": story["key"],
                                "test_cases": [self._test_case_to_dict(tc) for tc in test_cases],
                                "test_count": len(test_cases),
                                "coverage_level": coverage_level.value
                            }
                            
                            logger.info(f"Generated {len(test_cases)} test cases for task {task['key']}")
                            
                    except Exception as e:
                        logger.error(f"Error generating tests for task {task.get('key', 'unknown')}: {str(e)}")
                        results["errors"].append(f"Task {task.get('key', 'unknown')}: {str(e)}")
            
            # Calculate statistics
            total_story_tests = sum(data["test_count"] for data in story_tests.values())
            total_task_tests = sum(data["test_count"] for data in task_tests.values())
            total_tests = total_story_tests + total_task_tests
            
            # Compile results
            results.update({
                "success": True,
                "story_tests": story_tests,
                "task_tests": task_tests,
                "total_test_cases": total_tests,
                "test_statistics": {
                    "total_stories": len(story_tests),
                    "total_tasks": len(task_tests),
                    "total_story_tests": total_story_tests,
                    "total_task_tests": total_task_tests,
                    "average_tests_per_story": total_story_tests / len(story_tests) if story_tests else 0,
                    "average_tests_per_task": total_task_tests / len(task_tests) if task_tests else 0,
                    "coverage_level": coverage_level.value
                }
            })
            
            logger.info(f"Generated {total_tests} total test cases for epic {epic_key}")
            
        except Exception as e:
            results["success"] = False
            results["errors"].append(f"Unexpected error in test generation: {str(e)}")
            logger.error(f"Error in comprehensive test generation: {str(e)}")
        
        finally:
            results["execution_time_seconds"] = time.time() - start_time
            logger.info(f"Test generation completed in {results['execution_time_seconds']:.2f}s")
        
        return results
    
    def generate_story_tests(self, story_key: str, coverage_level: TestCoverageLevel = TestCoverageLevel.STANDARD) -> Dict[str, Any]:
        """
        Generate test cases for a specific story
        
        Args:
            story_key: JIRA story key
            coverage_level: Level of test coverage desired
            
        Returns:
            Test generation results for the story
        """
        logger.info(f"Generating tests for story {story_key} (coverage: {coverage_level.value})")
        
        start_time = time.time()
        results = {
            "success": False,
            "story_key": story_key,
            "coverage_level": coverage_level.value,
            "test_cases": [],
            "test_count": 0,
            "errors": [],
            "execution_time_seconds": 0
        }
        
        try:
            # Get story details from JIRA
            story_details = self.jira_client.get_ticket(story_key)
            
            if not story_details:
                results["errors"].append(f"Failed to retrieve story details: Story not found")
                return results
                
            # Wrap in expected format
            story_details = {"success": True, "issue": story_details}
            
            # Convert to story plan
            story_plan = self._convert_jira_story_to_plan(story_details["issue"])
            
            if not story_plan:
                results["errors"].append("Failed to convert JIRA story to story plan")
                return results
            
            # Determine domain context
            domain_context = self._extract_domain_context({}, story_details["issue"])
            
            # Generate test cases
            test_cases = self.test_generator.generate_story_test_cases(
                story=story_plan,
                coverage_level=coverage_level,
                domain_context=domain_context
            )
            
            # Compile results
            results.update({
                "success": True,
                "test_cases": [self._test_case_to_dict(tc) for tc in test_cases],
                "test_count": len(test_cases),
                "story_summary": story_plan.summary,
                "domain_context": domain_context
            })
            
            logger.info(f"Generated {len(test_cases)} test cases for story {story_key}")
            
            # ‚úÖ CRITICAL: Update JIRA with the generated test cases
            if test_cases:
                logger.info(f"Updating JIRA story {story_key} with {len(test_cases)} generated test cases")
                self._update_ticket_with_test_cases(story_key, test_cases)
            
        except Exception as e:
            results["success"] = False
            results["errors"].append(f"Unexpected error: {str(e)}")
            logger.error(f"Error generating story tests: {str(e)}")
        
        finally:
            results["execution_time_seconds"] = time.time() - start_time
        
        return results
    
    def generate_task_tests(self, task_key: str, coverage_level: TestCoverageLevel = TestCoverageLevel.STANDARD) -> Dict[str, Any]:
        """
        Generate test cases for a specific task
        
        Args:
            task_key: JIRA task key
            coverage_level: Level of test coverage desired
            
        Returns:
            Test generation results for the task
        """
        logger.info(f"Generating tests for task {task_key} (coverage: {coverage_level.value})")
        
        start_time = time.time()
        results = {
            "success": False,
            "task_key": task_key,
            "coverage_level": coverage_level.value,
            "test_cases": [],
            "test_count": 0,
            "errors": [],
            "execution_time_seconds": 0
        }
        
        try:
            # Get task details from JIRA
            task_details = self.jira_client.get_ticket(task_key)
            
            if not task_details:
                results["errors"].append(f"Failed to retrieve task details: Task not found")
                return results
                
            # Wrap in expected format
            task_details = {"success": True, "issue": task_details}
            
            # Convert to task plan
            task_plan = self._convert_jira_task_to_plan(task_details["issue"])
            
            if not task_plan:
                results["errors"].append("Failed to convert JIRA task to task plan")
                return results
            
            # Determine technical context
            technical_context = self._extract_technical_context(task_details["issue"])
            
            # Generate test cases
            test_cases = self.test_generator.generate_task_test_cases(
                task=task_plan,
                coverage_level=coverage_level,
                technical_context=technical_context
            )
            
            # Compile results
            results.update({
                "success": True,
                "test_cases": [self._test_case_to_dict(tc) for tc in test_cases],
                "test_count": len(test_cases),
                "task_summary": task_plan.summary,
                "technical_context": technical_context
            })
            
            logger.info(f"Generated {len(test_cases)} test cases for task {task_key}")
            
            # ‚úÖ CRITICAL: Update JIRA with the generated test cases
            if test_cases:
                logger.info(f"Updating JIRA task {task_key} with {len(test_cases)} generated test cases")
                self._update_ticket_with_test_cases(task_key, test_cases)
            
        except Exception as e:
            results["success"] = False
            results["errors"].append(f"Unexpected error: {str(e)}")
            logger.error(f"Error generating task tests: {str(e)}")
        
        finally:
            results["execution_time_seconds"] = time.time() - start_time
        
        return results
    
    def generate_context_aware_task_tests(self, 
                                        task_key: str, 
                                        coverage_level: TestCoverageLevel = TestCoverageLevel.STANDARD,
                                        technical_context: Optional[str] = None,
                                        include_documents: bool = True,
                                        custom_llm_client=None) -> Dict[str, Any]:
        """
        Generate context-aware test cases for a task with story and document context
        
        Args:
            task_key: JIRA task key
            coverage_level: Level of test coverage desired
            technical_context: Technical context for the task
            include_documents: Whether to include PRD/RFC context
            custom_llm_client: Custom LLM client if specified
            
        Returns:
            Enhanced test generation results with context information
        """
        logger.info(f"Generating context-aware tests for task {task_key} (coverage: {coverage_level.value})")
        
        start_time = time.time()
        results = {
            "success": False,
            "task_key": task_key,
            "coverage_level": coverage_level.value,
            "test_cases": [],
            "test_count": 0,
            "technical_context": technical_context,
            "story_context": None,
            "document_context": None,
            "context_sources": [],
            "errors": [],
            "execution_time_seconds": 0
        }
        
        try:
            # Create enhanced test generator with JIRA and Confluence clients
            from .enhanced_test_generator import EnhancedTestGenerator
            enhanced_generator = EnhancedTestGenerator(
                llm_client=custom_llm_client or self.llm_client,
                prompt_engine=self.prompt_engine,
                jira_client=self.jira_client,
                confluence_client=getattr(self, 'confluence_client', None)
            )
            
            # Generate context-aware test cases
            test_cases = enhanced_generator.generate_task_test_cases_with_story_context(
                task_key=task_key,
                coverage_level=coverage_level,
                technical_context=technical_context,
                include_documents=include_documents
            )
            
            # Get context information for response
            if hasattr(enhanced_generator, '_last_story_context'):
                results["story_context"] = enhanced_generator._last_story_context
                results["context_sources"].append("parent_story")
            
            if hasattr(enhanced_generator, '_last_document_context'):
                results["document_context"] = enhanced_generator._last_document_context
                if enhanced_generator._last_document_context and enhanced_generator._last_document_context.get('prd'):
                    results["context_sources"].append("prd")
                if enhanced_generator._last_document_context and enhanced_generator._last_document_context.get('rfc'):
                    results["context_sources"].append("rfc")
            
            # Compile results
            results.update({
                "success": True,
                "test_cases": [self._enhanced_test_case_to_dict(tc) for tc in test_cases],
                "test_count": len(test_cases)
            })
            
            logger.info(f"Generated {len(test_cases)} context-aware test cases for task {task_key}")
            
            # ‚úÖ CRITICAL: Update JIRA with the generated test cases
            if test_cases:
                logger.info(f"Updating JIRA task {task_key} with {len(test_cases)} context-aware test cases")
                self._update_ticket_with_test_cases(task_key, test_cases)
            
        except Exception as e:
            results["success"] = False
            results["errors"].append(f"Context-aware test generation error: {str(e)}")
            logger.error(f"Error generating context-aware task tests: {str(e)}")
            
            # Fallback to regular task test generation
            try:
                logger.info("Falling back to regular task test generation")
                fallback_results = self.generate_task_tests(task_key, coverage_level)
                if fallback_results["success"]:
                    results.update({
                        "success": True,
                        "test_cases": fallback_results["test_cases"],
                        "test_count": fallback_results["test_count"],
                        "technical_context": fallback_results.get("technical_context")
                    })
                    results["context_sources"].append("fallback_generation")
                    results["errors"].append("Used fallback generation due to context retrieval issues")
            except Exception as fallback_e:
                results["errors"].append(f"Fallback generation also failed: {str(fallback_e)}")
        
        finally:
            results["execution_time_seconds"] = time.time() - start_time
        
        return results
    
    def _convert_jira_story_to_plan(self, jira_story: Dict[str, Any]) -> Optional[StoryPlan]:
        """Convert JIRA story data to StoryPlan object"""
        try:
            # Extract acceptance criteria from description or custom fields
            acceptance_criteria = self._extract_acceptance_criteria(jira_story)
            
            # Handle structured description (Atlassian Document Format)
            description_field = jira_story.get("description", "")
            if isinstance(description_field, dict):
                description = self._extract_text_from_adf(description_field)
            else:
                description = description_field or ''
            
            return StoryPlan(
                summary=jira_story.get("summary", ""),
                description=description,
                acceptance_criteria=acceptance_criteria,
                story_points=jira_story.get("story_points", 1)
            )
        except Exception as e:
            logger.error(f"Error converting JIRA story to plan: {str(e)}")
            return None
    
    def _convert_jira_task_to_plan(self, jira_task: Dict[str, Any]) -> Optional[TaskPlan]:
        """Convert JIRA task data to TaskPlan object"""
        try:
            # Extract task scopes from description
            scopes = self._extract_task_scopes(jira_task)
            
            return TaskPlan(
                summary=jira_task.get("summary", ""),
                purpose=jira_task.get("description", ""),
                scopes=scopes,
                expected_outcomes=[jira_task.get("summary", "")],
                estimated_hours=8  # Default estimate
            )
        except Exception as e:
            logger.error(f"Error converting JIRA task to plan: {str(e)}")
            return None
    
    def _extract_acceptance_criteria(self, jira_story: Dict[str, Any]) -> List[AcceptanceCriteria]:
        """Extract acceptance criteria from JIRA story"""
        try:
            description = jira_story.get("description", "")
            criteria = []
            
            # Look for Given/When/Then patterns in description
            lines = description.split('\n')
            current_criteria = {}
            
            for line in lines:
                line = line.strip()
                if line.lower().startswith("given"):
                    current_criteria["given"] = line[5:].strip()
                elif line.lower().startswith("when"):
                    current_criteria["when"] = line[4:].strip()
                elif line.lower().startswith("then"):
                    current_criteria["then"] = line[4:].strip()
                    
                    # Complete criteria found
                    if all(key in current_criteria for key in ["given", "when", "then"]):
                        criteria.append(AcceptanceCriteria(
                            scenario=f"Scenario {len(criteria) + 1}",
                            given=current_criteria["given"],
                            when=current_criteria["when"],
                            then=current_criteria["then"]
                        ))
                        current_criteria = {}
            
            # If no proper criteria found, create a default one
            if not criteria:
                criteria.append(AcceptanceCriteria(
                    scenario="Default scenario",
                    given="User accesses the system",
                    when="User performs the action",
                    then="Expected functionality works correctly"
                ))
            
            return criteria
            
        except Exception as e:
            logger.error(f"Error extracting acceptance criteria: {str(e)}")
            return []
    
    def _extract_task_scopes(self, jira_task: Dict[str, Any]) -> List[TaskScope]:
        """Extract task scopes from JIRA task"""
        try:
            description = jira_task.get("description", "")
            summary = jira_task.get("summary", "")
            
            # Create basic scope from summary
            scopes = [
                TaskScope(
                    description=summary,
                    deliverable=f"Implementation of {summary}",
                    category="implementation"
                )
            ]
            
            return scopes
            
        except Exception as e:
            logger.error(f"Error extracting task scopes: {str(e)}")
            return []
    
    def _extract_domain_context(self, epic_info: Dict[str, Any], story_info: Dict[str, Any]) -> Optional[str]:
        """Extract domain context from epic and story information"""
        try:
            # Look for domain keywords in epic and story descriptions
            text_to_analyze = f"{epic_info.get('summary', '')} {epic_info.get('description', '')} {story_info.get('summary', '')} {story_info.get('description', '')}"
            text_lower = text_to_analyze.lower()
            
            # Domain detection patterns
            domain_patterns = {
                "financial": ["payment", "money", "financial", "bank", "transaction", "billing"],
                "healthcare": ["health", "medical", "patient", "hipaa", "clinical"],
                "ecommerce": ["shop", "cart", "order", "product", "purchase", "ecommerce"],
                "security": ["auth", "security", "login", "permission", "encrypt"]
            }
            
            for domain, keywords in domain_patterns.items():
                if any(keyword in text_lower for keyword in keywords):
                    return domain
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting domain context: {str(e)}")
            return None
    
    def _extract_technical_context(self, task_info: Dict[str, Any]) -> Optional[str]:
        """Extract technical context from task information"""
        try:
            text_to_analyze = f"{task_info.get('summary', '')} {task_info.get('description', '')}"
            text_lower = text_to_analyze.lower()
            
            # Technical context detection patterns
            tech_patterns = {
                "api": ["api", "endpoint", "rest", "service", "request", "response"],
                "database": ["database", "sql", "table", "query", "data", "persistence"],
                "ui": ["ui", "interface", "frontend", "component", "view", "page"],
                "microservice": ["microservice", "service", "distributed", "communication"]
            }
            
            for tech_context, keywords in tech_patterns.items():
                if any(keyword in text_lower for keyword in keywords):
                    return tech_context
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting technical context: {str(e)}")
            return None
    
    def _test_case_to_dict(self, test_case: TestCase) -> Dict[str, Any]:
        """Convert TestCase object to dictionary"""
        return {
            "title": test_case.title,
            "type": test_case.type,
            "description": test_case.description,
            "expected_result": test_case.expected_result
        }
    
    def _enhanced_test_case_to_dict(self, test_case: TestCase) -> Dict[str, Any]:
        """Convert TestCase object to enhanced dictionary with all Gherkin fields"""
        result = {
            "title": test_case.title,
            "type": test_case.type,
            "description": test_case.description,
            "expected_result": test_case.expected_result
        }
        
        # Add optional fields if they exist
        if hasattr(test_case, 'priority') and test_case.priority:
            result["priority"] = test_case.priority
        else:
            result["priority"] = "P2"  # Default priority
            
        if hasattr(test_case, 'traceability') and test_case.traceability:
            result["traceability"] = test_case.traceability
        else:
            result["traceability"] = ""
            
        if hasattr(test_case, 'precondition') and test_case.precondition:
            result["precondition"] = test_case.precondition
        else:
            result["precondition"] = ""
            
        if hasattr(test_case, 'steps') and test_case.steps:
            # Convert steps list to formatted string
            if isinstance(test_case.steps, list):
                result["test_steps"] = '\n'.join(test_case.steps)
            else:
                result["test_steps"] = str(test_case.steps)
        else:
            # Try to extract Gherkin steps from description
            result["test_steps"] = self._extract_gherkin_steps(test_case.description)
        
        return result
    
    def _extract_gherkin_steps(self, description: str) -> str:
        """Extract or format Gherkin steps from test description"""
        try:
            # Look for existing Gherkin format
            if any(keyword in description.lower() for keyword in ['given', 'when', 'then', 'and']):
                return description
            
            # If no Gherkin format, create a simple one
            return f"Given the system is in the required state\nWhen {description}\nThen the expected result should be achieved"
        except Exception:
            return description
    
    def _extract_text_from_adf(self, adf_content: Dict[str, Any]) -> str:
        """Extract plain text from Atlassian Document Format (ADF) content"""
        try:
            if not isinstance(adf_content, dict):
                return str(adf_content)
            
            text_parts = []
            
            def extract_text_recursive(node):
                if isinstance(node, dict):
                    # Handle text nodes
                    if node.get('type') == 'text':
                        text_parts.append(node.get('text', ''))
                    # Handle other nodes with content
                    elif 'content' in node:
                        for child in node['content']:
                            extract_text_recursive(child)
                    # Handle nodes with text directly
                    elif 'text' in node:
                        text_parts.append(node['text'])
                elif isinstance(node, list):
                    for item in node:
                        extract_text_recursive(item)
                elif isinstance(node, str):
                    text_parts.append(node)
            
            extract_text_recursive(adf_content)
            return ' '.join(text_parts).strip()
            
        except Exception as e:
            logger.warning(f"Error extracting text from ADF content: {str(e)}")
            return str(adf_content)[:500]  # Fallback to truncated string representation
    
    # =============================================================================
    # ENHANCED TEST GENERATION INTEGRATION METHODS
    # =============================================================================
    
    def generate_test_cases_for_stories(self, 
                                      stories: List[StoryPlan], 
                                      coverage_level: TestCoverageLevel = TestCoverageLevel.STANDARD,
                                      include_in_story_objects: bool = True) -> Dict[str, List[TestCase]]:
        """
        Generate comprehensive test cases for a list of stories
        
        Args:
            stories: List of story plans to generate tests for
            coverage_level: Level of test coverage desired
            include_in_story_objects: Whether to add test cases directly to story objects
            
        Returns:
            Dictionary mapping story summaries to test cases
        """
        logger.info(f"Generating test cases for {len(stories)} stories (coverage: {coverage_level.value})")
        
        story_tests = {}
        
        for story in stories:
            try:
                # Generate test cases using enhanced test generator
                test_cases = self.test_generator.generate_story_test_cases(
                    story=story,
                    coverage_level=coverage_level,
                    domain_context=self._detect_domain_context(story)
                )
                
                story_tests[story.summary] = test_cases
                
                # Add test cases to story object if requested
                if include_in_story_objects:
                    story.test_cases = test_cases
                
                logger.info(f"Generated {len(test_cases)} test cases for story: {story.summary}")
                
            except Exception as e:
                logger.error(f"Error generating test cases for story {story.summary}: {str(e)}")
                story_tests[story.summary] = []
        
        return story_tests
    
    def generate_test_cases_for_tasks(self, 
                                    tasks: List[TaskPlan], 
                                    coverage_level: TestCoverageLevel = TestCoverageLevel.STANDARD,
                                    include_in_task_objects: bool = True,
                                    custom_llm_client: Optional['LLMClient'] = None) -> Dict[str, List[TestCase]]:
        """
        Generate comprehensive test cases for a list of tasks with enhanced context
        
        Args:
            tasks: List of task plans to generate tests for
            coverage_level: Level of test coverage desired
            include_in_task_objects: Whether to add test cases directly to task objects
            custom_llm_client: Optional custom LLM client to use for test generation
            
        Returns:
            Dictionary mapping task summaries to test cases
        """
        logger.info(f"Generating test cases for {len(tasks)} tasks (coverage: {coverage_level.value})")
        
        task_tests = {}
        
        for task in tasks:
            try:
                logger.info(f"Starting enhanced test generation for task: {task.summary}")
                
                # Use custom LLM client if provided, otherwise use default test generator
                if custom_llm_client:
                    logger.info(f"Using custom LLM client for test generation: {custom_llm_client}")
                    custom_test_generator = EnhancedTestGenerator(
                        custom_llm_client,
                        self.prompt_engine,
                        jira_client=self.jira_client,
                        confluence_client=self.confluence_client
                    )
                    test_cases = custom_test_generator.generate_task_test_cases(
                        task=task,
                        coverage_level=coverage_level,
                        technical_context=self._detect_technical_context(task)
                    )
                else:
                    # Generate test cases using default enhanced test generator with context
                    test_cases = self.test_generator.generate_task_test_cases(
                        task=task,
                        coverage_level=coverage_level,
                        technical_context=self._detect_technical_context(task)
                    )
                
                task_tests[task.summary] = test_cases
                
                # Add test cases to task object if requested
                if include_in_task_objects:
                    task.test_cases = test_cases
                    logger.info(f"Enhanced test cases set on task object for: {task.summary}")
                
                logger.info(f"Generated {len(test_cases)} enhanced test cases for task: {task.summary}")
                
            except Exception as e:
                import traceback
                logger.error(f"CRITICAL: Enhanced test generation failed for task {task.summary}: {str(e)}")
                logger.error(f"Full traceback: {traceback.format_exc()}")
                logger.error(f"Task will keep original basic test cases from TeamBasedTaskGenerator")
                # Don't modify task.test_cases on failure - keep original basic test cases
                task_tests[task.summary] = []
        
        return task_tests
    
    def generate_context_aware_task_tests(self,
                                        task_keys: List[str],
                                        coverage_level: TestCoverageLevel = TestCoverageLevel.STANDARD,
                                        include_documents: bool = True) -> Dict[str, Dict[str, Any]]:
        """
        Generate context-aware test cases for existing JIRA tasks using enhanced extraction
        
        Args:
            task_keys: List of JIRA task keys
            coverage_level: Level of test coverage desired
            include_documents: Whether to include PRD/RFC context
            
        Returns:
            Dictionary mapping task keys to test generation results
        """
        logger.info(f"Generating context-aware test cases for {len(task_keys)} tasks")
        
        results = {}
        
        for task_key in task_keys:
            try:
                # Use enhanced test generator with full story and document context
                test_cases = self.test_generator.generate_task_test_cases_with_story_context(
                    task_key=task_key,
                    coverage_level=coverage_level,
                    technical_context=None,  # Will be auto-detected
                    include_documents=include_documents
                )
                
                results[task_key] = {
                    "success": True,
                    "test_cases": test_cases,
                    "test_count": len(test_cases),
                    "coverage_level": coverage_level.value,
                    "has_document_context": include_documents
                }
                
                logger.info(f"Generated {len(test_cases)} context-aware test cases for task {task_key}")
                
            except Exception as e:
                logger.error(f"Error generating context-aware tests for task {task_key}: {str(e)}")
                results[task_key] = {
                    "success": False,
                    "error": str(e),
                    "test_cases": [],
                    "test_count": 0
                }
        
        return results
    
    def plan_epic_with_test_generation(self, 
                                     context: PlanningContext,
                                     generate_story_tests: bool = True,
                                     generate_task_tests: bool = True,
                                     test_coverage_level: TestCoverageLevel = TestCoverageLevel.STANDARD) -> PlanningResult:
        """
        Complete epic planning with automatic test case generation
        
        Args:
            context: Planning context with epic key and options
            generate_story_tests: Whether to generate test cases for stories
            generate_task_tests: Whether to generate test cases for tasks
            test_coverage_level: Level of test coverage for generated tests
            
        Returns:
            PlanningResult with generated plan including test cases
        """
        logger.info(f"Planning epic {context.epic_key} with test generation (stories: {generate_story_tests}, tasks: {generate_task_tests})")
        
        start_time = time.time()
        
        try:
            # Step 1: Generate basic epic plan
            basic_result = self.plan_epic_complete(context)
            
            if not basic_result.success or not basic_result.epic_plan:
                logger.error("Basic epic planning failed, cannot generate tests")
                return basic_result
            
            # Step 2: Generate test cases for stories if requested
            if generate_story_tests and basic_result.epic_plan.stories:
                logger.info("Generating test cases for stories...")
                story_tests = self.generate_test_cases_for_stories(
                    stories=basic_result.epic_plan.stories,
                    coverage_level=test_coverage_level,
                    include_in_story_objects=True
                )
                
                # Add test generation summary to result
                basic_result.summary_stats["story_tests"] = {
                    "total_stories_with_tests": len([s for s in story_tests.values() if s]),
                    "total_story_test_cases": sum(len(tests) for tests in story_tests.values()),
                    "coverage_level": test_coverage_level.value
                }
            
            # Step 3: Generate test cases for tasks if requested
            if generate_task_tests and basic_result.epic_plan.stories:
                logger.info("Generating test cases for tasks...")
                all_tasks = []
                for story in basic_result.epic_plan.stories:
                    all_tasks.extend(story.tasks or [])
                
                if all_tasks:
                    task_tests = self.generate_test_cases_for_tasks(
                        tasks=all_tasks,
                        coverage_level=test_coverage_level,
                        include_in_task_objects=True
                    )
                    
                    # Add test generation summary to result
                    basic_result.summary_stats["task_tests"] = {
                        "total_tasks_with_tests": len([t for t in task_tests.values() if t]),
                        "total_task_test_cases": sum(len(tests) for tests in task_tests.values()),
                        "coverage_level": test_coverage_level.value
                    }
            
            # Update execution time
            basic_result.execution_time_seconds = time.time() - start_time
            
            logger.info(f"Epic planning with test generation completed for {context.epic_key}")
            return basic_result
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Error in epic planning with test generation: {str(e)}")
            
            return PlanningResult(
                epic_key=context.epic_key,
                mode=context.mode,
                success=False,
                errors=[f"Epic planning with test generation failed: {str(e)}"],
                execution_time_seconds=execution_time
            )
    
    def _detect_domain_context(self, story: StoryPlan) -> Optional[str]:
        """Detect domain context from story content"""
        try:
            text_to_analyze = f"{story.summary} {story.description}".lower()
            
            domain_patterns = {
                "financial": ["payment", "money", "financial", "bank", "transaction", "billing", "invoice"],
                "healthcare": ["health", "medical", "patient", "hipaa", "clinical", "treatment"],
                "ecommerce": ["shop", "cart", "order", "product", "purchase", "inventory", "catalog"],
                "security": ["auth", "security", "login", "permission", "encrypt", "access", "token"],
                "api": ["api", "endpoint", "service", "integration", "webhook", "rest"]
            }
            
            for domain, keywords in domain_patterns.items():
                if any(keyword in text_to_analyze for keyword in keywords):
                    return domain
            
            return None
            
        except Exception as e:
            logger.warning(f"Error detecting domain context: {str(e)}")
            return None
    
    def _detect_technical_context(self, task: TaskPlan) -> Optional[str]:
        """Detect technical context from task content"""
        try:
            text_to_analyze = f"{task.summary} {task.purpose}".lower()
            
            tech_patterns = {
                "api": ["api", "endpoint", "rest", "service", "request", "response", "integration"],
                "database": ["database", "sql", "table", "query", "data", "persistence", "migration"],
                "ui": ["ui", "frontend", "interface", "component", "view", "page", "react", "angular"],
                "backend": ["backend", "server", "business logic", "processing", "validation"],
                "microservice": ["microservice", "service", "distributed", "communication", "messaging"]
            }
            
            for tech_context, keywords in tech_patterns.items():
                if any(keyword in text_to_analyze for keyword in keywords):
                    return tech_context
            
            return None
            
        except Exception as e:
            logger.warning(f"Error detecting technical context: {str(e)}")
            return None
    
    def sync_stories_from_prd_table(self, epic_key: str, prd_content: Dict[str, Any],
                                    existing_ticket_action: str = "skip", dry_run: bool = False) -> 'PlanningResult':
        """
        Sync stories from PRD table to JIRA
        
        Args:
            epic_key: Epic key to associate stories with
            prd_content: PRD page data from Confluence
            existing_ticket_action: Action for existing tickets: "skip", "update", or "error"
            
        Returns:
            PlanningResult with synced stories
        """
        import time
        from .planning_models import PlanningResult, OperationMode, EpicPlan
        from .prd_story_parser import PRDStoryParser
        
        start_time = time.time()
        logger.info(f"Syncing stories from PRD table for epic {epic_key}")
        
        try:
            # Parse stories from PRD table
            parser = PRDStoryParser()
            stories = parser.parse_stories_from_prd_content(prd_content, epic_key)
            
            if not stories:
                logger.warning(f"No stories parsed from PRD content for epic {epic_key}")
                # Debug: Check if PRD content has body structure
                has_body = 'body' in prd_content
                has_storage = prd_content.get('body', {}).get('storage', {}) if has_body else {}
                has_value = 'value' in has_storage if has_storage else False
                logger.debug(f"PRD content structure - has_body: {has_body}, has_storage: {bool(has_storage)}, has_value: {has_value}")
                return PlanningResult(
                    epic_key=epic_key,
                    mode=OperationMode.PLANNING,
                    success=False,
                    errors=["No stories found in PRD table"],
                    execution_time_seconds=time.time() - start_time
                )
            
            logger.info(f"Parsed {len(stories)} stories from PRD table")
            
            # First, check PRD table for existing JIRA links (preferred method)
            stories_with_jira_keys = {}
            for story in stories:
                if story.key:
                    stories_with_jira_keys[story.key] = story
                    logger.debug(f"Found JIRA key in PRD table for story: {story.key} - {story.summary}")
            
            # Fallback: Check JIRA for existing stories by title
            existing_stories = self._get_epic_stories(epic_key)
            existing_story_titles = {}
            
            if existing_stories:
                for story_key in existing_stories:
                    try:
                        story_data = self.jira_client.get_ticket(story_key)
                        if story_data:
                            title = story_data.get('fields', {}).get('summary', '')
                            existing_story_titles[title.lower()] = story_key
                    except Exception as e:
                        logger.warning(f"Error getting existing story {story_key}: {e}")
            
            # Filter stories based on existing_ticket_action
            stories_to_create = []
            stories_to_update = []
            skipped_stories = []
            story_metadata = {}  # Track metadata: story_key -> {source, action_taken, was_updated, jira_url}
            jira_server_url = self.jira_client.server_url.rstrip('/')
            
            for story in stories:
                # Priority 1: Check if story has JIRA key from PRD table
                if story.key and story.key in stories_with_jira_keys:
                    existing_key = story.key
                    jira_url = f"{jira_server_url}/browse/{existing_key}"
                    
                    if existing_ticket_action == "error":
                        return PlanningResult(
                            epic_key=epic_key,
                            mode=OperationMode.PLANNING,
                            success=False,
                            errors=[f"Story '{story.summary}' already exists as {existing_key} (found in PRD table)"],
                            execution_time_seconds=time.time() - start_time
                        )
                    elif existing_ticket_action == "update":
                        # Story already has key from PRD, will be synced
                        stories_to_update.append(story)
                        story_metadata[existing_key] = {
                            'source': 'prd_table',
                            'action_taken': 'update',
                            'was_updated': None,  # Will be set after sync
                            'jira_url': jira_url
                        }
                    else:  # skip
                        skipped_stories.append(story.summary)
                        story_metadata[existing_key] = {
                            'source': 'prd_table',
                            'action_taken': 'skipped',
                            'was_updated': False,
                            'jira_url': jira_url
                        }
                else:
                    # Priority 2: Check JIRA API for existing stories by title (fallback)
                    story_title_lower = story.summary.lower()
                    if story_title_lower in existing_story_titles:
                        existing_key = existing_story_titles[story_title_lower]
                        jira_url = f"{jira_server_url}/browse/{existing_key}"
                        
                        if existing_ticket_action == "error":
                            return PlanningResult(
                                epic_key=epic_key,
                                mode=OperationMode.PLANNING,
                                success=False,
                                errors=[f"Story '{story.summary}' already exists as {existing_key}"],
                                execution_time_seconds=time.time() - start_time
                            )
                        elif existing_ticket_action == "update":
                            story.key = existing_key
                            stories_to_update.append(story)
                            story_metadata[existing_key] = {
                                'source': 'jira_api',
                                'action_taken': 'update',
                                'was_updated': None,  # Will be set after sync
                                'jira_url': jira_url
                            }
                        else:  # skip
                            skipped_stories.append(story.summary)
                            story_metadata[existing_key] = {
                                'source': 'jira_api',
                                'action_taken': 'skipped',
                                'was_updated': False,
                                'jira_url': jira_url
                            }
                    else:
                        # New story - needs to be created
                        stories_to_create.append(story)
            
            # During dry run: Generate UUIDs and add placeholders to PRD
            if dry_run and stories_to_create and self.confluence_client:
                import uuid
                from .prd_table_updater import add_uuid_placeholder_to_row, find_row_by_uuid
                from bs4 import BeautifulSoup
                
                try:
                    # Get current HTML content
                    html_content = prd_content.get('body', {}).get('storage', {}).get('value', '')
                    if html_content:
                        soup = BeautifulSoup(html_content, 'html.parser')
                        
                        # Find story table to get row indices
                        story_table = None
                        for tbl in soup.find_all("table"):
                            headers = [th.get_text(" ", strip=True) for th in tbl.find_all("th")]
                            if not headers:
                                first_row = tbl.find('tr')
                                if first_row:
                                    headers = [cell.get_text(" ", strip=True) for cell in first_row.find_all(['th', 'td'])]
                            
                            if headers:
                                headers_lower = [h.lower() for h in headers]
                                has_user_story = any("user story" in h or "user-story" in h for h in headers_lower)
                                has_acceptance_criteria = any("acceptance criteria" in h or "acceptance-criteria" in h for h in headers_lower)
                                if has_user_story and has_acceptance_criteria:
                                    story_table = tbl
                                    break
                        
                        if story_table:
                            rows = story_table.find_all("tr")
                            if len(rows) >= 2:
                                # Get header to find story column
                                header_row = rows[0]
                                header_cells = header_row.find_all(['th', 'td'])
                                header_texts = [cell.get_text(" ", strip=True) for cell in header_cells]
                                
                                story_col_idx = None
                                for idx, header in enumerate(header_texts):
                                    if "user story" in header.lower() or "user-story" in header.lower():
                                        story_col_idx = idx
                                        break
                                
                                if story_col_idx is not None:
                                    updated_html = html_content
                                    uuid_added_count = 0
                                    
                                    # Generate UUIDs and add to PRD rows
                                    for story in stories_to_create:
                                        # Skip if story already has a key
                                        if story.key:
                                            continue
                                        
                                        # Use existing UUID if already set (from previous dry run), otherwise generate new one
                                        if not story.prd_row_uuid:
                                            row_uuid = str(uuid.uuid4())
                                            story.prd_row_uuid = row_uuid
                                        else:
                                            row_uuid = story.prd_row_uuid
                                            logger.debug(f"Reusing existing UUID for story: {story.summary}")
                                        
                                        # Find matching row by story summary
                                        matched_row_idx = None
                                        for row_idx, row in enumerate(rows[1:], 0):  # Start from 0 for data rows
                                            cells = row.find_all("td")
                                            if story_col_idx < len(cells):
                                                story_cell = cells[story_col_idx]
                                                cell_text = story_cell.get_text(" ", strip=True)
                                                # Match by checking if story summary is in cell text
                                                if story.summary[:50].lower() in cell_text.lower() or cell_text[:50].lower() in story.summary.lower():
                                                    matched_row_idx = row_idx
                                                    break
                                        
                                        if matched_row_idx is not None:
                                            # Add UUID placeholder to PRD row
                                            updated_html = add_uuid_placeholder_to_row(
                                                updated_html,
                                                matched_row_idx,
                                                row_uuid
                                            )
                                            if updated_html:
                                                uuid_added_count += 1
                                                logger.info(f"Added UUID placeholder to PRD row {matched_row_idx} for story: {story.summary}")
                                            else:
                                                logger.warning(f"Failed to add UUID placeholder to PRD row {matched_row_idx}")
                                        else:
                                            logger.warning(f"Could not find matching row in PRD table for story: {story.summary}")
                                    
                                    # Update PRD page if we added any UUIDs
                                    if uuid_added_count > 0 and updated_html != html_content:
                                        page_id = prd_content.get('id')
                                        if page_id:
                                            # Get current page version
                                            url = f"{self.confluence_client.server_url}/rest/api/content/{page_id}"
                                            params = {'expand': 'version'}
                                            response = self.confluence_client.session.get(url, params=params, timeout=30)
                                            response.raise_for_status()
                                            full_page_data = response.json()
                                            current_version = full_page_data.get('version', {}).get('number', 1)
                                            
                                            # Update the page
                                            success = self.confluence_client.update_page_content(page_id, updated_html, current_version)
                                            if success:
                                                logger.info(f"Successfully updated PRD page with {uuid_added_count} UUID placeholders (dry run)")
                                                # Update prd_content with new HTML for subsequent operations
                                                prd_content['body']['storage']['value'] = updated_html
                                            else:
                                                logger.warning("Failed to update Confluence page with UUID placeholders")
                except Exception as e:
                    logger.warning(f"Error adding UUID placeholders during dry run: {e}")
                    import traceback
                    logger.debug(traceback.format_exc())
                    # Continue even if UUID addition fails
            
            # Create/update stories (only if not dry run)
            created_tickets = {"stories": []}
            
            if not dry_run:
                if stories_to_create:
                    created = self._create_story_tickets(stories_to_create)
                    created_tickets["stories"].extend(created.get("stories", []))
                
                if stories_to_update:
                    for story in stories_to_update:
                        try:
                            # Sync story with PRD content
                            was_updated = self._sync_story_with_prd(story, story.key)
                            created_tickets["stories"].append(story.key)
                            # Update metadata
                            if story.key in story_metadata:
                                story_metadata[story.key]['was_updated'] = was_updated
                        except Exception as e:
                            logger.error(f"Error syncing story {story.key}: {e}")
                            if story.key in story_metadata:
                                story_metadata[story.key]['was_updated'] = False
                
                # Update PRD table with JIRA links for updated stories
                if stories_to_update and self.confluence_client:
                    updated_story_keys = [story.key for story in stories_to_update if story.key]
                    if updated_story_keys:
                        self._update_prd_table_with_story_links_uuid(
                            prd_content=prd_content,
                            stories=stories_to_update,
                            created_story_keys=updated_story_keys
                        )
                
                # Update PRD table with JIRA links for newly created stories
                if stories_to_create and self.confluence_client:
                    created_keys = created_tickets.get("stories", [])
                    # Use UUID-based update if available, otherwise fallback to fuzzy matching
                    self._update_prd_table_with_story_links_uuid(
                        prd_content=prd_content,
                        stories=stories_to_create,
                        created_story_keys=created_keys
                    )
                    # Add metadata for newly created stories
                    for i, story_key in enumerate(created_keys):
                        if i < len(stories_to_create):
                            jira_url = f"{jira_server_url}/browse/{story_key}"
                            story_metadata[story_key] = {
                                'source': 'newly_created',
                                'action_taken': 'created',
                                'was_updated': False,
                                'jira_url': jira_url
                            }
            
            # Build epic plan with all stories (including skipped ones for display)
            # The 'stories' variable contains all parsed stories, regardless of whether they're created/updated/skipped
            # This ensures story_details will show all stories from the PRD
            logger.info(f"Building epic plan with {len(stories)} stories (total parsed: {len(stories_to_create)} to create, {len(stories_to_update)} to update, {len(skipped_stories)} skipped)")
            
            # Ensure we're using the original stories list (all parsed stories)
            if not stories:
                logger.warning(f"No stories available for epic plan, but {len(stories_to_create) + len(stories_to_update) + len(skipped_stories)} were processed")
            
            epic_plan = EpicPlan(
                epic_key=epic_key,
                epic_title=f"Epic {epic_key}",
                stories=stories  # All parsed stories, regardless of action taken
            )
            
            # Verify epic_plan was created with stories
            if epic_plan.stories:
                logger.info(f"Epic plan created successfully with {len(epic_plan.stories)} stories")
            else:
                logger.warning(f"Epic plan created but has no stories! Original stories count: {len(stories)}")
            
            execution_time = time.time() - start_time
            
            warnings = []
            if skipped_stories:
                warnings.append(f"Skipped {len(skipped_stories)} existing stories: {', '.join(skipped_stories[:5])}")
            
            return PlanningResult(
                epic_key=epic_key,
                mode=OperationMode.PLANNING,
                success=True,
                created_tickets=created_tickets,
                epic_plan=epic_plan,
                warnings=warnings,
                execution_time_seconds=execution_time,
                story_metadata=story_metadata if story_metadata else None
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Error syncing stories from PRD for {epic_key}: {str(e)}")
            return PlanningResult(
                epic_key=epic_key,
                mode=OperationMode.PLANNING,
                success=False,
                errors=[str(e)],
                execution_time_seconds=execution_time
            )
    
    def _get_epic_stories(self, epic_key: str) -> List[str]:
        """Get all stories linked to an epic"""
        try:
            jql = f'"Epic Link" = {epic_key} AND issuetype = Story'
            issues = self.jira_client.jira.search_issues(jql, maxResults=1000)
            return [issue.key for issue in issues]
        except Exception as e:
            logger.warning(f"Error getting stories for epic {epic_key}: {str(e)}")
            return []
    
    def _update_story_ticket(self, story: 'StoryPlan'):
        """Update an existing story ticket"""
        if not story.key:
            raise ValueError("Story key is required for update")
        
        logger.info(f"Updating story ticket {story.key}")
        
        # Update description
        description_adf = story.format_description_for_jira_adf()
        
        update_data = {
            "fields": {
                "description": description_adf
            }
        }
        
        self.jira_client.jira.update_issue(story.key, update_data)
        logger.info(f"Updated story ticket {story.key}")
        
        # Handle image attachments if description contains images
        # Pass Confluence server URL for image attachment support
        confluence_server_url = None
        if self.confluence_client:
            confluence_server_url = self.confluence_client.server_url
        
        self.jira_client._attach_images_from_description(story.key, story.description, confluence_server_url)
    
    def _sync_story_with_prd(self, story: StoryPlan, jira_key: str) -> bool:
        """
        Sync existing JIRA story ticket with PRD content
        
        Compares PRD story content with JIRA ticket and updates JIRA if differences found.
        
        Args:
            story: StoryPlan from PRD
            jira_key: JIRA ticket key
            
        Returns:
            True if sync successful, False otherwise
        """
        try:
            # Fetch current JIRA ticket
            jira_ticket = self.jira_client.get_ticket(jira_key)
            if not jira_ticket:
                logger.warning(f"Could not fetch JIRA ticket {jira_key} for sync")
                return False
            
            # Compare and update if needed
            updates = []
            fields = jira_ticket.get('fields', {})
            
            # Compare summary/title
            current_summary = fields.get('summary', '')
            if story.summary != current_summary:
                # Update summary (with truncation if needed)
                try:
                    self.jira_client.update_ticket_summary(jira_key, story.summary)
                    updates.append("summary")
                except Exception as e:
                    logger.warning(f"Failed to update summary for {jira_key}: {e}")
            
            # Compare description
            current_description_raw = fields.get('description')
            current_description = ""
            if current_description_raw:
                # Extract text from ADF format if present
                if isinstance(current_description_raw, dict):
                    current_description = self._extract_text_from_adf(current_description_raw)
                else:
                    current_description = str(current_description_raw)
            
            formatted_description = story.format_description()
            # Normalize for comparison (remove extra whitespace)
            current_desc_normalized = ' '.join(current_description.split())
            formatted_desc_normalized = ' '.join(formatted_description.split())
            
            if formatted_desc_normalized != current_desc_normalized:
                # Update description
                try:
                    # update_ticket_description accepts markdown string and converts to ADF internally
                    formatted_description = story.format_description()
                    self.jira_client.update_ticket_description(jira_key, formatted_description, dry_run=False)
                    updates.append("description")
                    
                    # Handle image attachments if description contains images
                    confluence_server_url = None
                    if self.confluence_client:
                        confluence_server_url = self.confluence_client.server_url
                    self.jira_client._attach_images_from_description(jira_key, story.description, confluence_server_url)
                except Exception as e:
                    logger.warning(f"Failed to update description for {jira_key}: {e}")
            
            if updates:
                logger.info(f"Synced JIRA ticket {jira_key}: updated {', '.join(updates)}")
                return True  # Actually updated
            else:
                logger.debug(f"JIRA ticket {jira_key} is already in sync with PRD")
                return False  # No updates needed
            
        except Exception as e:
            logger.error(f"Error syncing story {jira_key} with PRD: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return False
    
    def _update_prd_table_with_story_links(
        self, 
        prd_content: Dict[str, Any], 
        stories: List[StoryPlan],
        created_story_keys: List[str]
    ) -> bool:
        """
        Update PRD table with JIRA ticket links for newly created stories
        
        Args:
            prd_content: PRD page data from Confluence
            stories: List of stories that were created
            created_story_keys: List of JIRA ticket keys that were created (in same order as stories)
            
        Returns:
            True if update successful, False otherwise
        """
        if not self.confluence_client:
            logger.warning("Confluence client not available, cannot update PRD table")
            return False
        
        try:
            page_id = prd_content.get('id')
            if not page_id:
                logger.warning("PRD content missing page ID, cannot update")
                return False
            
            # Get current HTML content
            html_content = prd_content.get('body', {}).get('storage', {}).get('value', '')
            if not html_content:
                logger.warning("PRD content missing HTML, cannot update")
                return False
            
            # Get JIRA server URL for constructing links
            jira_server_url = self.jira_client.server_url.rstrip('/')
            
            # Update HTML for each created story
            from .prd_table_updater import update_story_row_with_jira_link
            
            updated_html = html_content
            stories_updated = 0
            
            # Map stories to their row indices
            # We need to match stories back to their PRD table rows
            # Since we don't have row indices stored, we'll match by story summary
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Find story table
            story_table = None
            for tbl in soup.find_all("table"):
                headers = [th.get_text(" ", strip=True) for th in tbl.find_all("th")]
                if not headers:
                    first_row = tbl.find('tr')
                    if first_row:
                        headers = [cell.get_text(" ", strip=True) for cell in first_row.find_all(['th', 'td'])]
                
                if headers:
                    headers_lower = [h.lower() for h in headers]
                    has_user_story = any("user story" in h or "user-story" in h for h in headers_lower)
                    has_acceptance_criteria = any("acceptance criteria" in h or "acceptance-criteria" in h for h in headers_lower)
                    if has_user_story and has_acceptance_criteria:
                        story_table = tbl
                        break
            
            if not story_table:
                logger.warning("Could not find story table in PRD for updating")
                return False
            
            # Find rows and match stories
            rows = story_table.find_all("tr")
            if len(rows) < 2:  # Need at least header + 1 data row
                logger.warning("Story table has no data rows")
                return False
            
            # Get header to find story column
            header_row = rows[0]
            header_cells = header_row.find_all(['th', 'td'])
            header_texts = [cell.get_text(" ", strip=True) for cell in header_cells]
            
            story_col_idx = None
            for idx, header in enumerate(header_texts):
                if "user story" in header.lower() or "user-story" in header.lower():
                    story_col_idx = idx
                    break
            
            if story_col_idx is None:
                logger.warning("Could not find 'User Story' column in PRD table")
                return False
            
            # Match stories to rows and update
            for story_idx, story in enumerate(stories):
                if story_idx >= len(created_story_keys):
                    break
                
                story_key = created_story_keys[story_idx]
                if not story_key:
                    continue
                
                # Find matching row by story summary
                matched_row_idx = None
                for row_idx, row in enumerate(rows[1:], 0):  # Start from 0 for data rows
                    cells = row.find_all("td")
                    if story_col_idx < len(cells):
                        story_cell = cells[story_col_idx]
                        cell_text = story_cell.get_text(" ", strip=True)
                        # Match by checking if story summary is in cell text
                        if story.summary[:50].lower() in cell_text.lower() or cell_text[:50].lower() in story.summary.lower():
                            matched_row_idx = row_idx
                            break
                
                if matched_row_idx is not None:
                    jira_url = f"{jira_server_url}/browse/{story_key}"
                    updated_html = update_story_row_with_jira_link(
                        updated_html,
                        matched_row_idx,
                        story_key,
                        jira_url
                    )
                    if updated_html:
                        stories_updated += 1
                        logger.info(f"Updated PRD table row {matched_row_idx} with JIRA link: {story_key}")
                    else:
                        logger.warning(f"Failed to update PRD table row {matched_row_idx} for story {story_key}")
                else:
                    logger.warning(f"Could not find matching row in PRD table for story: {story.summary}")
            
            # Update Confluence page if we made any changes
            if stories_updated > 0 and updated_html != html_content:
                # Get current page version
                page_data = self.confluence_client._get_page_by_id(page_id)
                if page_data:
                    # Get version from full page data
                    url = f"{self.confluence_client.server_url}/rest/api/content/{page_id}"
                    params = {'expand': 'version'}
                    response = self.confluence_client.session.get(url, params=params, timeout=30)
                    response.raise_for_status()
                    full_page_data = response.json()
                    current_version = full_page_data.get('version', {}).get('number', 1)
                    
                    # Update the page
                    success = self.confluence_client.update_page_content(page_id, updated_html, current_version)
                    if success:
                        logger.info(f"Successfully updated PRD page with {stories_updated} JIRA ticket links")
                        return True
                    else:
                        logger.warning("Failed to update Confluence page with PRD table changes")
                        return False
                else:
                    logger.warning("Could not fetch page version for PRD update")
                    return False
            else:
                logger.debug("No PRD table updates needed")
                return True
                
        except Exception as e:
            logger.warning(f"Error updating PRD table with JIRA links: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            # Don't fail story creation if PRD update fails
            return False
    
    def _update_prd_table_with_story_links_uuid(
        self,
        prd_content: Dict[str, Any],
        stories: List[StoryPlan],
        created_story_keys: List[str]
    ) -> bool:
        """
        Update PRD table with JIRA ticket links using UUID placeholders (preferred method)
        Falls back to fuzzy matching if UUID not found
        
        Args:
            prd_content: PRD page data from Confluence
            stories: List of stories that were created
            created_story_keys: List of JIRA ticket keys that were created (in same order as stories)
            
        Returns:
            True if update successful, False otherwise
        """
        if not self.confluence_client:
            logger.warning("Confluence client not available, cannot update PRD table")
            return False
        
        try:
            page_id = prd_content.get('id')
            if not page_id:
                logger.warning("PRD content missing page ID, cannot update")
                return False
            
            # Get current HTML content
            html_content = prd_content.get('body', {}).get('storage', {}).get('value', '')
            if not html_content:
                logger.warning("PRD content missing HTML, cannot update")
                return False
            
            # Get JIRA server URL for constructing links
            jira_server_url = self.jira_client.server_url.rstrip('/')
            
            # Update HTML for each created story
            from .prd_table_updater import find_row_by_uuid, replace_uuid_with_jira_link, update_story_row_with_jira_link
            
            updated_html = html_content
            stories_updated = 0
            
            # Process each story
            for story_idx, story in enumerate(stories):
                if story_idx >= len(created_story_keys):
                    break
                
                story_key = created_story_keys[story_idx]
                if not story_key:
                    continue
                
                jira_url = f"{jira_server_url}/browse/{story_key}"
                row_updated = False
                
                # Try UUID-based matching first
                if story.prd_row_uuid:
                    row_idx = find_row_by_uuid(updated_html, story.prd_row_uuid)
                    if row_idx is not None:
                        # Replace UUID with JIRA link
                        updated_html = replace_uuid_with_jira_link(
                            updated_html,
                            row_idx,
                            story_key,
                            jira_url
                        )
                        if updated_html:
                            stories_updated += 1
                            row_updated = True
                            logger.info(f"Updated PRD table row {row_idx} with JIRA link using UUID: {story_key}")
                        else:
                            logger.warning(f"Failed to replace UUID with JIRA link for {story_key}")
                    else:
                        logger.warning(f"UUID {story.prd_row_uuid} not found in PRD table for story {story_key}, falling back to fuzzy matching")
                
                # Fallback to fuzzy matching if UUID not found or not available
                if not row_updated:
                    # Use existing fuzzy matching logic
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(updated_html, 'html.parser')
                    
                    # Find story table
                    story_table = None
                    for tbl in soup.find_all("table"):
                        headers = [th.get_text(" ", strip=True) for th in tbl.find_all("th")]
                        if not headers:
                            first_row = tbl.find('tr')
                            if first_row:
                                headers = [cell.get_text(" ", strip=True) for cell in first_row.find_all(['th', 'td'])]
                        
                        if headers:
                            headers_lower = [h.lower() for h in headers]
                            has_user_story = any("user story" in h or "user-story" in h for h in headers_lower)
                            has_acceptance_criteria = any("acceptance criteria" in h or "acceptance-criteria" in h for h in headers_lower)
                            if has_user_story and has_acceptance_criteria:
                                story_table = tbl
                                break
                    
                    if story_table:
                        rows = story_table.find_all("tr")
                        if len(rows) >= 2:
                            # Get header to find story column
                            header_row = rows[0]
                            header_cells = header_row.find_all(['th', 'td'])
                            header_texts = [cell.get_text(" ", strip=True) for cell in header_cells]
                            
                            story_col_idx = None
                            for idx, header in enumerate(header_texts):
                                if "user story" in header.lower() or "user-story" in header.lower():
                                    story_col_idx = idx
                                    break
                            
                            if story_col_idx is not None:
                                # Find matching row by story summary
                                matched_row_idx = None
                                for row_idx, row in enumerate(rows[1:], 0):  # Start from 0 for data rows
                                    cells = row.find_all("td")
                                    if story_col_idx < len(cells):
                                        story_cell = cells[story_col_idx]
                                        cell_text = story_cell.get_text(" ", strip=True)
                                        # Match by checking if story summary is in cell text
                                        if story.summary[:50].lower() in cell_text.lower() or cell_text[:50].lower() in story.summary.lower():
                                            matched_row_idx = row_idx
                                            break
                                
                                if matched_row_idx is not None:
                                    updated_html = update_story_row_with_jira_link(
                                        updated_html,
                                        matched_row_idx,
                                        story_key,
                                        jira_url
                                    )
                                    if updated_html:
                                        stories_updated += 1
                                        logger.info(f"Updated PRD table row {matched_row_idx} with JIRA link (fuzzy match): {story_key}")
                                    else:
                                        logger.warning(f"Failed to update PRD table row {matched_row_idx} for story {story_key}")
                                else:
                                    logger.warning(f"Could not find matching row in PRD table for story: {story.summary}")
            
            # Update Confluence page if we made any changes
            if stories_updated > 0 and updated_html != html_content:
                # Get current page version
                page_data = self.confluence_client._get_page_by_id(page_id)
                if page_data:
                    # Get version from full page data
                    url = f"{self.confluence_client.server_url}/rest/api/content/{page_id}"
                    params = {'expand': 'version'}
                    response = self.confluence_client.session.get(url, params=params, timeout=30)
                    response.raise_for_status()
                    full_page_data = response.json()
                    current_version = full_page_data.get('version', {}).get('number', 1)
                    
                    # Update the page
                    success = self.confluence_client.update_page_content(page_id, updated_html, current_version)
                    if success:
                        logger.info(f"Successfully updated PRD page with {stories_updated} JIRA ticket links")
                        return True
                    else:
                        logger.warning("Failed to update Confluence page with PRD table changes")
                        return False
                else:
                    logger.warning("Could not fetch page version for PRD update")
                    return False
            else:
                logger.debug("No PRD table updates needed")
                return True
                
        except Exception as e:
            logger.warning(f"Error updating PRD table with JIRA links (UUID method): {e}")
            import traceback
            logger.debug(traceback.format_exc())
            # Don't fail story creation if PRD update fails
            return False
    
    def _update_prd_table_for_story(
        self,
        story_summary: str,
        story_key: str,
        epic_key: str,
        prd_row_uuid: Optional[str] = None
    ) -> bool:
        """
        Update PRD table with JIRA link for a single story.
        
        Args:
            story_summary: Story summary/title to match against PRD rows
            story_key: JIRA ticket key (e.g., "STORY-123")
            epic_key: Parent epic key to get PRD from
            prd_row_uuid: Optional UUID for exact row matching (from dry run)
            
        Returns:
            True if PRD was updated successfully, False otherwise
        """
        if not self.confluence_client:
            logger.debug("Confluence client not available, cannot update PRD table")
            return False
        
        try:
            # Get epic issue to access PRD custom field
            epic_issue = self.jira_client.get_ticket(epic_key)
            if not epic_issue:
                logger.debug(f"Epic {epic_key} not found, cannot get PRD")
                return False
            
            # Get PRD URL from epic
            prd_url = self._get_custom_field_value(epic_issue, 'PRD')
            if not prd_url:
                logger.debug(f"Epic {epic_key} does not have PRD custom field set")
                return False
            
            # Get PRD content
            prd_content = self.confluence_client.get_page_content(prd_url)
            if not prd_content:
                logger.warning(f"Failed to retrieve PRD content from {prd_url}")
                return False
            
            page_id = prd_content.get('id')
            if not page_id:
                logger.warning("PRD content missing page ID, cannot update")
                return False
            
            # Get current HTML content
            html_content = prd_content.get('body', {}).get('storage', {}).get('value', '')
            if not html_content:
                logger.warning("PRD content missing HTML, cannot update")
                return False
            
            # Get JIRA server URL for constructing links
            jira_server_url = self.jira_client.server_url.rstrip('/')
            jira_url = f"{jira_server_url}/browse/{story_key}"
            
            from .prd_table_updater import find_row_by_uuid, replace_uuid_with_jira_link, update_story_row_with_jira_link
            from bs4 import BeautifulSoup
            
            updated_html = html_content
            row_updated = False
            
            # Try UUID-based matching first if UUID provided
            if prd_row_uuid:
                row_idx = find_row_by_uuid(html_content, prd_row_uuid)
                if row_idx is not None:
                    # Replace UUID with JIRA link
                    updated_html = replace_uuid_with_jira_link(
                        html_content,
                        row_idx,
                        story_key,
                        jira_url
                    )
                    if updated_html:
                        row_updated = True
                        logger.info(f"Updated PRD table row {row_idx} with JIRA link using UUID: {story_key}")
                    else:
                        logger.warning(f"Failed to replace UUID with JIRA link for {story_key}")
                else:
                    logger.warning(f"UUID {prd_row_uuid} not found in PRD table for story {story_key}, falling back to fuzzy matching")
            
            # Fallback to fuzzy matching if UUID not found or not provided
            if not row_updated:
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # Find story table
                story_table = None
                for tbl in soup.find_all("table"):
                    headers = [th.get_text(" ", strip=True) for th in tbl.find_all("th")]
                    if not headers:
                        first_row = tbl.find('tr')
                        if first_row:
                            headers = [cell.get_text(" ", strip=True) for cell in first_row.find_all(['th', 'td'])]
                    
                    if headers:
                        headers_lower = [h.lower() for h in headers]
                        has_user_story = any("user story" in h or "user-story" in h for h in headers_lower)
                        has_acceptance_criteria = any("acceptance criteria" in h or "acceptance-criteria" in h for h in headers_lower)
                        if has_user_story and has_acceptance_criteria:
                            story_table = tbl
                            break
                
                if not story_table:
                    logger.warning("Could not find story table in PRD for updating")
                    return False
                
                rows = story_table.find_all("tr")
                if len(rows) < 2:
                    logger.warning("Story table has no data rows")
                    return False
                
                # Get header to find story column
                header_row = rows[0]
                header_cells = header_row.find_all(['th', 'td'])
                header_texts = [cell.get_text(" ", strip=True) for cell in header_cells]
                
                story_col_idx = None
                for idx, header in enumerate(header_texts):
                    if "user story" in header.lower() or "user-story" in header.lower():
                        story_col_idx = idx
                        break
                
                if story_col_idx is None:
                    logger.warning("Could not find 'User Story' column in PRD table")
                    return False
                
                # Find matching row by story summary
                matched_row_idx = None
                for row_idx, row in enumerate(rows[1:], 0):  # Start from 0 for data rows
                    cells = row.find_all("td")
                    if story_col_idx < len(cells):
                        story_cell = cells[story_col_idx]
                        cell_text = story_cell.get_text(" ", strip=True)
                        # Match by checking if story summary is in cell text
                        if story_summary[:50].lower() in cell_text.lower() or cell_text[:50].lower() in story_summary.lower():
                            matched_row_idx = row_idx
                            break
                
                if matched_row_idx is not None:
                    updated_html = update_story_row_with_jira_link(
                        html_content,
                        matched_row_idx,
                        story_key,
                        jira_url
                    )
                    if updated_html:
                        row_updated = True
                        logger.info(f"Updated PRD table row {matched_row_idx} with JIRA link (fuzzy match): {story_key}")
                    else:
                        logger.warning(f"Failed to update PRD table row {matched_row_idx} for story {story_key}")
                else:
                    logger.warning(f"Could not find matching row in PRD table for story: {story_summary}")
                    return False
            
            # Update Confluence page if we made changes
            if row_updated and updated_html != html_content:
                # Get current page version
                url = f"{self.confluence_client.server_url}/rest/api/content/{page_id}"
                params = {'expand': 'version'}
                response = self.confluence_client.session.get(url, params=params, timeout=30)
                response.raise_for_status()
                full_page_data = response.json()
                current_version = full_page_data.get('version', {}).get('number', 1)
                
                # Update the page
                success = self.confluence_client.update_page_content(page_id, updated_html, current_version)
                if success:
                    logger.info(f"Successfully updated PRD page with JIRA link for {story_key}")
                    return True
                else:
                    logger.warning("Failed to update Confluence page with PRD table changes")
                    return False
            
            return row_updated
                
        except Exception as e:
            logger.warning(f"Error updating PRD table for story {story_key}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            # Don't fail story creation if PRD update fails
            return False