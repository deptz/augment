# JIRA Configuration (Required)
JIRA_SERVER_URL=https://your-company.atlassian.net
JIRA_USERNAME=your-email@company.com
JIRA_API_TOKEN=your-jira-api-token
JIRA_PRD_CUSTOM_FIELD=customfield_10001
JIRA_RFC_CUSTOM_FIELD=
JIRA_TEST_CASE_CUSTOM_FIELD=
JIRA_MANDAYS_CUSTOM_FIELD=

# Bitbucket Configuration (Optional)
# Used for fetching pull requests and commit information
# For multiple workspaces, use comma-separated list:
BITBUCKET_WORKSPACES=workspace1,workspace2
# OR for single workspace (backward compatible):
# BITBUCKET_WORKSPACE=your-workspace
BITBUCKET_EMAIL=your-email@company.com
BITBUCKET_API_TOKEN=your-atlassian-api-token

# Confluence Configuration (Optional)
# Used for fetching PRD/RFC content from Confluence pages
CONFLUENCE_SERVER_URL=https://your-company.atlassian.net/wiki
CONFLUENCE_USERNAME=your-email@company.com
CONFLUENCE_API_TOKEN=your-confluence-api-token

# LLM Configuration (Required)
# Choose one provider: openai, claude, gemini, or kimi
LLM_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key
OPENAI_MODEL=gpt-5-mini

# Anthropic (Claude) Configuration
ANTHROPIC_API_KEY=your-anthropic-api-key
ANTHROPIC_MODEL=claude-sonnet-4-5

# Google (Gemini) Configuration
GOOGLE_API_KEY=your-google-api-key
GOOGLE_MODEL=gemini-2.0-flash-exp

# Moonshot AI (KIMI) Configuration
MOONSHOT_API_KEY=your-moonshot-api-key
MOONSHOT_MODEL=moonshot-v1-8k

# LLM Advanced Configuration (Optional)
LLM_SYSTEM_PROMPT=You are a technical documentation assistant specializing in creating structured Jira ticket descriptions from historical development artifacts.
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=

# Processing Configuration (Optional)
MAX_CONCURRENT_REQUESTS=5
REQUEST_TIMEOUT=30
BATCH_SIZE=10
INCLUDE_CODE_ANALYSIS=true
STORY_DESCRIPTION_MAX_LENGTH=800
STORY_DESCRIPTION_SUMMARY_THRESHOLD=1200
MAX_TASKS_PER_STORY=10

# Authentication Configuration (Optional)
# Enable HTTP Basic Authentication for API endpoints
AUTH_ENABLED=false
AUTH_USERNAME=admin
AUTH_PASSWORD_HASH=

# Redis Configuration (Required for background jobs)
# Used for ARQ background job queue
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# Worker Configuration (Optional)
# ARQ worker process settings
WORKER_MAX_JOBS=10
WORKER_JOB_TIMEOUT=3600
WORKER_KEEP_RESULT=3600

# Sprint Planning Configuration (Optional)
SPRINT_DURATION_DAYS=14
TEAM_CAPACITY_DAYS=10.0
AUTO_CREATE_SPRINTS=false

# Environment Settings (Optional)
ENVIRONMENT=development

# Team Member Database (Optional)
# Custom database path (absolute or relative to project root)
# Default: data/team_members.db
TEAM_MEMBER_DB_PATH=

#CORS Allowed Origin setting
# Comma-separated list of allowed origins for Cross-Origin Resource Sharing
# Example: http://localhost:5173,http://localhost:3000,https://example.com
# If not set, defaults to common localhost ports for development
CORS_ALLOWED_ORIGINS=http://localhost:5173,http://localhost:3000
# OpenCode Integration (Optional)
# Enable code-aware LLM generation using OpenCode Docker containers

# OpenCode Integration (Optional)
# Enable code-aware LLM generation using OpenCode Docker containers
OPENCODE_ENABLED=false
OPENCODE_DOCKER_IMAGE=ghcr.io/anomalyco/opencode
OPENCODE_MAX_CONCURRENT=2
OPENCODE_MAX_REPOS=5
OPENCODE_TIMEOUT=20
OPENCODE_CLONE_TIMEOUT=300
OPENCODE_SHALLOW_CLONE=true
OPENCODE_MAX_RESULT_SIZE=10

# OpenCode-Specific LLM Configuration (REQUIRED - separate from main LLM config)
OPENCODE_LLM_PROVIDER=claude  # REQUIRED: Options: openai, claude, gemini, kimi
OPENCODE_ANTHROPIC_API_KEY=sk-ant-api03-...  # REQUIRED if provider=claude
OPENCODE_ANTHROPIC_MODEL=claude-haiku-4-5  # REQUIRED if provider=claude
# OR for other providers:
# OPENCODE_OPENAI_API_KEY=sk-...  # REQUIRED if provider=openai
# OPENCODE_OPENAI_MODEL=gpt-5-mini  # REQUIRED if provider=openai
# OPENCODE_GOOGLE_API_KEY=...  # REQUIRED if provider=gemini
# OPENCODE_GOOGLE_MODEL=gemini-2.5-flash  # REQUIRED if provider=gemini
# OPENCODE_MOONSHOT_API_KEY=...  # REQUIRED if provider=kimi
# OPENCODE_MOONSHOT_MODEL=moonshot-v1-8k  # REQUIRED if provider=kimi

OPENCODE_LLM_TEMPERATURE=0.7  # Optional: defaults to 0.7
OPENCODE_LLM_MAX_TOKENS=  # Optional: uses provider defaults if not set

# Git Credentials for OpenCode (Required for private repositories)
# These credentials are used when cloning repositories via HTTPS
# Works with any Git provider: Bitbucket, GitHub, GitLab, etc.
#
# For Bitbucket:
#   GIT_USERNAME = Your Bitbucket username or email
#   GIT_PASSWORD = Bitbucket App Password (create at: https://bitbucket.org/account/settings/app-passwords/)
#
# For GitHub:
#   GIT_USERNAME = Your GitHub username
#   GIT_PASSWORD = Personal Access Token (create at: https://github.com/settings/tokens)
#
# For GitLab:
#   GIT_USERNAME = Your GitLab username
#   GIT_PASSWORD = Personal Access Token (create at: https://gitlab.com/-/profile/personal_access_tokens)
#
GIT_USERNAME=
GIT_PASSWORD=
